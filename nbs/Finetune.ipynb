{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for finetuning BERT\n",
    "\n",
    "### The notebook is split up into the following sections:\n",
    "1. Clean data\n",
    "2. Split dataset into 80/20 split and tokenize\n",
    "3. Finetune BERT\n",
    "4. Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import ttest_ind\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in dataset\n",
    "df = pd.read_csv(\"../Scraping/clean_reddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "CasualConversation    4044\n",
       "depression            3864\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets do a quick pronoun count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== DEPRESSION ==========\n",
      "Total pronoun counts: Counter({'i': 91360, 'my': 25372, 'me': 15784, 'myself': 4715, 'mine': 142})\n",
      "Total posts: 3864\n",
      "Total pronouns: 137373\n",
      "Average per post: 35.55201863354037\n",
      "Median per post: 26\n",
      "\n",
      "========== CASUALCONVERSATION ==========\n",
      "Total pronoun counts: Counter({'i': 28393, 'my': 8376, 'me': 4450, 'myself': 713, 'mine': 154})\n",
      "Total posts: 4044\n",
      "Total pronouns: 42086\n",
      "Average per post: 10.40702274975272\n",
      "Median per post: 7\n"
     ]
    }
   ],
   "source": [
    "# First-person singular pronouns\n",
    "FIRST_PERSON_PRONOUNS = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "\n",
    "# Count total number of first-person pronouns in a single post\n",
    "def count_first_person(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    return sum(1 for t in tokens if t in FIRST_PERSON_PRONOUNS)\n",
    "\n",
    "# Count each individual pronoun\n",
    "def count_each_pronoun(text):\n",
    "    if not isinstance(text, str):\n",
    "        return Counter()\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    return Counter(t for t in tokens if t in FIRST_PERSON_PRONOUNS)\n",
    "\n",
    "# split by subreddit\n",
    "df_dep = df[df[\"subreddit\"] == \"depression\"]\n",
    "df_ctrl = df[df[\"subreddit\"] == \"CasualConversation\"]\n",
    "\n",
    "# total and per-post counts \n",
    "def analyze_group(df_group, name):\n",
    "    total_counter = Counter()\n",
    "    per_post_counts = []\n",
    "\n",
    "    for text in df_group[\"full_text\"]:\n",
    "        per_post_counts.append(count_first_person(text))\n",
    "        total_counter.update(count_each_pronoun(text))\n",
    "\n",
    "    print(f\"\\n========== {name.upper()} ==========\")\n",
    "    print(\"Total pronoun counts:\", total_counter)\n",
    "    print(\"Total posts:\", len(df_group))\n",
    "    print(\"Total pronouns:\", sum(per_post_counts))\n",
    "    print(\"Average per post:\", sum(per_post_counts) / len(per_post_counts))\n",
    "    print(\"Median per post:\", sorted(per_post_counts)[len(per_post_counts)//2])\n",
    "\n",
    "# Run analysis for both groups\n",
    "analyze_group(df_dep, \"Depression\")\n",
    "analyze_group(df_ctrl, \"CasualConversation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the length of the posts from the dataset from r/depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_35946/3604314048.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dep[\"char_len\"] = df_dep[\"full_text\"].str.len()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     3864.000000\n",
       "mean      1543.149845\n",
       "std       1548.748249\n",
       "min         43.000000\n",
       "25%        635.750000\n",
       "50%       1103.000000\n",
       "75%       1895.250000\n",
       "max      18017.000000\n",
       "Name: char_len, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dep[\"char_len\"] = df_dep[\"full_text\"].str.len()\n",
    "df_dep[\"char_len\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the length of the posts from the dataset from r/CasualConversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_35946/3401160515.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ctrl[\"char_len\"] = df_ctrl[\"full_text\"].str.len()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    4044.000000\n",
       "mean      748.936944\n",
       "std       667.918083\n",
       "min        80.000000\n",
       "25%       347.000000\n",
       "50%       542.000000\n",
       "75%       915.000000\n",
       "max      8676.000000\n",
       "Name: char_len, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ctrl[\"char_len\"] = df_ctrl[\"full_text\"].str.len()\n",
    "df_ctrl[\"char_len\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the max length from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I\\'m 18 and my life is a mess of regret. I need to get this off my chest...\\n\\nI don\\'t even know where to begin. I\\'m writing this because my mental health is in the gutter and I need to just put it all out there. Maybe someone can make sense of it, because I can\\'t.\\n\\nI was born in April 2006 in Lansing, Michigan. I\\'m a guy. Ever since I was young, I have always been into girls. Like, when I was in elementary school I used to, oh god... this is embarrassing... I would pretend I dropped something like a pencil or an eraser look under the table I was sat at to see under the girls\\' skirts in my class. I know, it\\'s disgusting.\\n\\nI grew up and had a good childhood. I was a chubby kid back then. Now, when I started highschool, I used to have no discipline and no sense of hygiene. I didn\\'t take showersâ€” only sometimes - like once a week or once every two weeks.\\n\\nI woke up, went to school, and didn\\'t use the toilets in school since I hated them and thought they were disgusting. I held my pee in until I got home. I also held my poop in, which is what caused my body to always be warm and that\\'s what caused my skin to go bad and made it fill with pimples. My parents warned me not to pop any pimples as they can cause permanent scarring and damage, but I was an idiot and around 11 or 12 years old back then so I ignored them and thought they were just making a bunch of stuff up.\\n\\nMy usual routine was: wake up, eat breakfast, go to school, don\\'t pay attention in class (since I was too \"cool\" for it), eat lunch during lunchtime, come home and play on my PlayStation 4 for long hours or use my iPad to play games, etc. Yes, I was spoiled. Also, I was the first child, the oldest one in the house with one younger brother. I was living the dream.\\n\\nI always used to hold in my poop, which is very dangerous, since I didn\\'t wanna waste my time on the toilet. Sometimes I couldn\\'t hold it so I had to use the toilet, like when I was excited about something or if I ate too much. It\\'s already pretty bad, I know, but things are about to go haywire so... yeah.\\n\\nIt was here where I started to watch \"spicy\" content. It wasn\\'t porn; I didn\\'t fully discover it. I started to watch these kissing prank videos, nude celebrities, etc. Then I got caught by my parents, which was embarrassing.\\n\\nI was also very, VERY horny at this stage of my life. During this time I was also bullied quite a lot. I was always the tallest in my grade. I started to develop a little mustache (little). I was a pretty chubby kid and got bullied and got called \"fat shit\" in school. Girls made fun of me, basically everyone did.\\n\\nSo my dad decided to put me on the treadmill so I lose weight because this extra weight wasn\\'t good for my health and plus he didn\\'t want to see his son be lazy everyday. He pushed me every day until in a month\\'s time I could jog at a pace of 8km/h for 40 minutes. I started to lose weight during this period and could eat almost anything since I was still growing and had a pretty fast metabolism and was always hungry and felt really hot (fast metabolism). It was amazing; and I was starting to get in shape.\\n\\nBut that\\'s when things took a turn. I was so horny. I was watching kissing pranks and I didn\\'t realize they were fake. I took some pointers from those videos and went up to actual women... God, this is... I have no words to describe this abomination of an act.\\n\\nI talked to them seductively and had a boner in front of them. I saw this woman walking into the park and followed her and talked to her. She was very kind so I thought kindness would mean I could touch her, so I touched her boob. She quickly threw my hand away and said \"No,\" and I still kept on doing it. She said she\\'ll report me to the police. The next day that same lady comes and visits my house, with her husband and told my parents everything.\\n\\nMy parents were devastated. Their face went pale. My heart was pounding a million times per second. I had this bad feeling or sensation wash up on me. Her husband threatened my parents, said they\\'ll report this to the police, and was very pissed.\\n\\nMy father, a high moral and intelligent, high honour good man who never had to apologize or beg someone like this before, begged him not to report me to the police. He went down to his knees and put his head on his shoes and cried (I know this because my mom told me after all this). I was sitting in my room listening. My dad was very pissed and angry and disappointed. He had so many emotions; he couldn\\'t believe his son could do this. He warned me not to do it again.\\n\\nNow keep in mind, I\\'m dumbâ€”like stone-age level dumb. Like, really dumb, even though nothing is wrong with my brain. So during this time, watching porn etc., I was looking at women\\'s vaginas and wondering where the penis is. I thought women had penises. I was 12, mind you. I was looking for videos of women with penises. I didn\\'t even learn that women have anuses too until I was 15. See, I didn\\'t focus in school, didn\\'t understand English very well, had limited knowledge and nothing about maths or science. I was basically a fuck-up.\\n\\nNow in my family there is this no-privacy rule where our room (me and my brother slept in the same room) had to have the door open always. So I had not a lot of time to watch porn. I had wet dreams during these periods and wondered why my pants suddenly get filled up with this sticky white liquid when I wake up (cum), or why I eject this white liquid when I watch porn. I thought there was something wrong with me or why I have a boner. I didn\\'t know what these were; I was new to this. I genuinely thought there is something wrong with me. We don\\'t usually have sex talk in my family; they don\\'t talk about these things openly.\\n\\nDuring this time, age 12-13 (which most of the things that happened are around), I had so many school crushes on girls. I lusted over them heavily but didn\\'t dare touch them or anything because I knew they would complain to the headmaster\\'s office and parents will be involved, etc.\\n\\nBut it was summer a few months later and I looked at them in a lustful and creepy way. They were my age, all in my class. I looked at their butts and got super horny. It was sports day and I looked at their boobs; they were slim fit and had good body. So I looked at these two sisters who wore tight leggings or tights and I could see their butt and kept looking when no one was seeing and fantasized having sex with them. They had a ponytail and were beautiful.\\n\\nBut then even worse happens. I did the same act again and started to call on girls and be like \"hey baby nice tits or sexy ass wanna fuck\" etc., like some disgusting rapist shit, and looked them lustfully up and down and they could see it. I started harassing girls, following them, and some even said they\\'ll report me.\\n\\nSo one day, from afar from a window, I started following a girl and looked from the window to see where she was going. And that\\'s when my dad caught me. He took me home and hit me multiple times with a belt and was very angry with me. I wet my pants cause of the beating (which was well deserved; he should have broken my face).\\n\\nNow the reason for this extreme horniness is because I didn\\'t know how to jerk off or masturbate. Kids this age know, but not Mr. Dumbo here (me).\\n\\nSo after this fucked up situation, my dad banned me for 3 months from video games, iPad, or any screen, etc. He also shaved my head during this time since it was summer (and my brother\\'s head too, but my younger brother could adapt. He was also punished for my actionsâ€”collective punishment, I guess).\\n\\nI would play chess, study, or do maths, go outside, bring groceries, etc.\\n\\nAnd also remember that ever since I was a child, like 6 or 8 years old, my mum used to hit me sometimes when she made me study and I got things wrong. Since my dumb brain couldn\\'t comprehend maths or couldn\\'t concentrate or learn my times tables, etc., she used to hit me with hangers or rulers (the bendy ones that could bend that hurt, but not like full force, just punishment). I also have a grandpa and dad who would criticize or scold me sometimes if I made some mistakes. The men did that.\\n\\nI had a friend who would come to my house sometimes and he was kind of like my best friend in elementary school until I left. When I went to high school I had no friends; I was a loner. I wanted to be friends with someone but they weren\\'t really my friends. There were these two guys: top of the class and heavy into video games, etc. I asked them for some math advice and walked with them to the school library or canteen during lunch break. I used to play video games with them; they added me to PSN (PlayStation Network) and we played online.\\n\\nBut yeah, my dad said since I was banned forever until I improved my grades, he\\'ll unban me from everything until I improve. So I did. I worked very, very hard to understand math (my weakness; I was like a grade 3 person in grade 8 when it came to maths, and everything else).\\n\\nSo I watched basics all of them for maths: what are numbers, how to add and subtract and multiply and divide and work with fractions, decimals, PEMDAS (I didn\\'t even know division properly), then times tables. I worked very hard and started to stay late after school (a thing I never did since I thought only nerds do this, since I\\'m not a nerd). I stayed late after school and asked teachers about my maths misconceptions and any help they could give. It paid off and I started getting 60% in my examsâ€”a huge jump from 7-10%. I also worked hard in physics class and paid attention and got 80% marks too.\\n\\nAnd my dad also told me to read at least for one hour a day a book, whether it\\'s a fiction or non-fiction. So I started reading Harry Potter, Diary of a Wimpy Kid, etc., and improved my English a lot. I used my phone dictionary (I also had a phone; my mum\\'s iPhone 5 was given down to me) and it had parental lock and everything so I could not surf the web or watch YouTube, only dictionary access. So I looked up words and what they meant and worked very hard. My English reading grade went from a level of a 9-year-old to a level of a 12.5-year-old in a span of 3 months. I was 13 then.\\n\\nSo they called my dad and told him this. He was very happy and took unbanned me so I could watch TV, use my laptop (he gave me a laptop for personal use and for school). He said there is gonna be a limit to video games. Like before, when I played video games whenever or however much I could, this time it was only for Fridays (when I come home from school), Saturdays and Sundays for 2 hours each day. And he also said he expects me to do my homework and not delay it or miss any deadlines. So I was always punctual to school, never missed homework, and mostly did my homework during lunch in the school library. If there was more, I finished it at home in under an hour. I was a good kid back then.\\n\\nNow my dad and my grandpa make me do things like bring them a glass of water or help them with something, etc., like normal things. But they also heavily criticized me and called me dumb and sometimes make fun of my brain.\\n\\nFor example, my grandpa will ask, \"Hey, tighten this screw here,\" or \"bring me something from the toolbox.\" It was either I couldn\\'t tighten the screw or find the thing he wants from the toolbox. Simple things like that resulted in me getting scolded when the item, for example, I was looking for was in the toolbox.\\n\\nAnother example: I accidentally opened the door without looking when there was a person walking on the side and hit him. My dad had to apologize to the person (he is a very kind and peaceful guy) but then he angrily looked at me and cursed at me, \"son of a bitch, you didn\\'t see him coming!\" Sometimes it results in a mild slap (not too hard).\\n\\nHe cares deeply about respect and honour and doesn\\'t take lightly when someone disrespects the family name, the religion, the country, the race, etc. He says to me and my brother we represent all these things. My dad is a short-tempered guy but a good guy nonetheless.\\n\\nNow fast forward. I completed high school and got decent grades. But now I\\'m 18 and I am free. I\\'m looking for a job and got a local job. Now I\\'m thinking about sex and love and girls and women, etc.\\n\\nI was heavy on porn and by then I had already realized women don\\'t have penises but vaginas. In these porn videos I saw a \"happy ending\" massage being given to a guy by an Asian woman. So I decided to check Asian parlours near me that offer nuru massage or happy ending. I found one on Google Maps with good reviews (which don\\'t mention any sexual experiences) and visited it one day. I said \"happy ending.\"\\n\\nThere was this beautiful mid-30 or late early 40-year-old looking woman and she was hot. It was an experience like nothing I ever felt. I never kissed anyone well; I did that day. I never felt that women would be attracted to me and this was first time I was seeing someone attracted to me. Then after this day I tried to learn how to masturbate so I\\'m not horny any more so finally I discovered masturbation at age 18 (how dumb can you get to discover something that a 14 yr old or a 12 yr old should be discovering).\\n\\nI then regretted everything I did and felt guilty as heck. But I never got any attention from girls. After some time, she wasn\\'t there and left the massage parlour so didn\\'t work anymore. I used to go every 2 months to visit the massage parlour. I wanted affection. I wanted to be loved, to be cared for, and I wanted to be lusted over since I never experienced that before. That\\'s why I never forgot this experience.\\n\\nBut then after some time I started to get addicted. I saw other masseuses; all were Chinese masseuses since it was an oriental massage parlour. Eventually, I wished I didn\\'t. I had sex after 1 year of visiting it for the first time. Then after 3-4 months I had sex with escorts.\\n\\nI was devastated, empty, soulless that I could do such a thing. I was also masturbating this time but I tried to restrain myself but couldn\\'t. I wanted a physical person, a body I could touch and interact with who would willingly kiss me and have sex with me. So I kept visiting and drained my own money.\\n\\nI can never fully recover from this. I regret this deeply and have so, so, so much guilt. I wish I didn\\'t do this; this is not me. I wish I had found a good girl and made her my gf like everyone else does.\\n\\nAlso during this time I was learning to drive and still haven\\'t because you might say the problem is instructors or the teachers, but the whole problem is me. I had 4 instructors and still failed my driving test. On top of that, I had 60 hours of driving lessons. People pass in 30; by now I probably have 65 hours and I\\'m still not ready for driving. That\\'s how dumb I am.\\n\\nThe only thing I\\'m good at is running, which I can improve on and be better than most people since I\\'m above average.\\n\\nI wish I had friends to guide me, a gf, and some help. I\\'m alone. My parents are just parents, not friends. I love them but they\\'re not like friends; you don\\'t share everything with parents.\\n\\nI disappointed my family. I disappointed myself and the world. My dad once said \"You\\'re a burden to this world.\" I hate my life. I feel depressed.\\n\\nI can\\'t get a gf now. I\\'m done because I don\\'t like to lie. I would probably tell her all about me so she knows who she\\'s getting into because maybe she thought of me as someone else, but then the truth would probably hurt so better to let her know beforehand.\\n\\nAlso, I\\'m not a bad person internally. I wanna help people. I wanna give charity and probably create my own organization in the medical sector helping people for free, etc.\\n\\nThere\\'s way more to the story but I\\'m so tired of this. I\\'ll write the entire second part later, but here\\'s a summary of my life: I have no job, I\\'m not tall, average or slightly above average looking, average or slightly below average dick, not richâ€”I\\'m broke. I want help. Am I a bad person? I disappointed my parents a lot.\\n\\nI don\\'t understand things. Like the other day my dad told me to build this simple, super simple DIY science kit for 7+ ages that he ordered from Temu and I couldn\\'t figure out how to fix it. I tried fixing a DualShock 4 controller, a PS4 controller, but couldn\\'t. I\\'m a loser. I need help. I need advice.\\n\\nI don\\'t feel that guilty or disappointed in myself as I\\'m writing; I feel nothing. I feel empty, soulless. But some days when I think about or reflect on what I did in my life, I wish I had died and never been such an asshole. My mental health isn\\'t even strong right now.\\n\\nI\\'m working hard right now and trying my best to improve myself, learn new skills, become a better person etc. But I feel lonely. I don\\'t wanna die alone - that\\'s one of my greatest fear. But I guess after all I did, I deserve to die alone with no family, no kids, no one to take care of me and a fuck-up in the end.\\n\\nI also fell in love with a K-pop idol. And before you say it\\'s lust, it\\'s not. I rarely fall in love but when I do I fall hard. I fell in love with this idol because she\\'s so kind, nice, and beautiful. My type is someone who looks like they came from the skies like an angel (like a Greek goddess). She\\'s half-Korean, half-white Australian and she looks so beautiful. I wasn\\'t initially attracted to her when I first saw her, but as she did more Insta lives and she smiles and talks so kindly and is so nice, and she shares so many similar traits to me (not those I listed, not those asshole traits)â€”like her favourite color is the same as mine, her favourite movie same as mine, her personality is happy and jumping and sunshine and very uplifting, etc.\\n\\nI want her, yet I know I don\\'t deserve her. I am also starting to see my hair balding (not yet but I can see signs). I\\'m done for. No girl will like me. I feel like killing myself but I can\\'t. Then sometimes I don\\'t wanna die and feel like it\\'s my duty to educate people and help people like me never make the same mistakes I did. I\\'m a lost cause, I\\'m empty, I\\'m soulless...\\n\\nI\\'m a...disappointment.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = df_dep[\"full_text\"].apply(count_first_person).nlargest(1).index\n",
    "df_dep.loc[idx, \"full_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tips on how to find hobbies as a 22 year old female, plus me spilling other mumbo jumbo!\\n\\nHey yall, bit of a random post to make but it feels necessary and I need advice. I often feel stuck with my free time because I donâ€™t have any time consuming hobbies. Sure, I enjoy to be crafty and creative and do set aside time to do things I enjoy such as making jewelry, thrifting, doing my makeup routine every morning, drawing, baking/cooking, etc. Butâ€¦ this stuff doesnâ€™t isolate a large portion of my time everyday. Iâ€™m a very social motivated person, and my favorite thing to do is spend time with my finance and my friends. Itâ€™s hard because we donâ€™t live together yet, so the concept of being within each otherâ€™s company even whilst doing separate things isnâ€™t a reality yet. Aside from work, we spend almost all our free time together, which I absolutely adore. \\n\\nWhen I was younger, I really enjoyed video games. My fiancÃ© (21m) still loves to play video games in his free time, which makes me happy for him that he never lost the love for them like I did. However, I find it hard that I donâ€™t have anything in my life apart from him that I can get lost in for hours like he can with his video games. It feels like I donâ€™t know how to show up for myself in a way, because it used to be so easy to figure out what to do with my free time. Sure, I enjoy watching movies/shows when Iâ€™m in the mood, but this isnâ€™t typically something I can do for hours. I am the type of person that prefers to be out and about in her free time, whether that be driving around or visiting stores I like, or even going somewhere new. Itâ€™s all about the adventure for me. And whatâ€™s great is my fiancÃ© and I love to go fishing together, which is something weâ€™ve been really into the last couple months and has been very consistent in our lives. Itâ€™s an amazing hobby weâ€™ve gotten into together. So fishing is the most time consuming hobby Iâ€™ve discovered and been able to get into as my 22 year old self, but again this is something I enjoy doing WITH my fiancÃ©. Pretty much, my goal here is to get advice on how I can enjoy my alone time better, and find things to do that I genuinely can get lost in for hours without necessarily needing company. Even if I was more into gaming, Iâ€™d prefer to play with people, which is what my fiancÃ© does with his friends. So maybe hobbies are better shared with people? Iâ€™m not sure if me striving for hobbies I can do by myself to feel more whole is realistic or not, but Iâ€™m curious if thereâ€™s other people out there in the same boat as me. Because whenever Iâ€™m alone I rather be with people (fiancÃ©, friends, family). It feels more like a preference than anything, but I wish I could find the free time just for myself, me spending time with me and only me. Donâ€™t get me wrong, people need people, so my goal isnâ€™t to shut people out. But itâ€™s hard because part of the reason I feel like itâ€™s difficult for me to feel more inspired and motivated to find things to do is due to my environment. I still live at home, and my sister is either at work or doing something with her fiance, so we donâ€™t have a lot of time together anymore. And my mom is someone Iâ€™m not super close to and have tried to involve her more in my life but she never reciprocated that need. And my dad and I are super close, but heâ€™s someone who will be off for a selected period of time and then off working as an assistant director, so our time together is inconsistent. Maybe itâ€™s also because Iâ€™m getting older, but my goals are aligned with more grown up things. Like I want my own place with my fiance, him and I are starting nursing next week, I obviously adore to have a family of my own, and all my mind can focus on is these things. I donâ€™t really make time for ME anymore, in a weird way I feel. But something I love about his house is that thereâ€™s so much life to it. Thereâ€™s animals, and my fiancÃ©â€™s niece is over a lot, and overall his family is very respectful of our relationship, so I prefer spending time there as opposed to my house. My parents are old school so thereâ€™s weird restrictions about where we can hang at my house (like we canâ€™t go in my room, even with the door open my parents are too nosy), and overall my parents are very much helicopter parents, and I donâ€™t feel they take my relationship that seriously so it feels weird being around them with my fiance, which sucks. But at my fiancÃ©s house I get the love from my future mother in law that I never got with my mom, his family has a cat which is my favorite pet to have and my cat died a few years ago. My house just feels empty and lonely compared to his. And this feels relevant to my story because sure, my room has grown to be my safe space in my house, but something about my fiancÃ©s house feels safer overall, which I think gives my fiance a sense of comfort to be able to indulge in his interests better. I mean, us being able to spend time together alone at his house in his room is a huge part of why our relationship feels more real and comfortable as opposed to my house, because thereâ€™s a sense of welcoming and trust from his family about me and our relationship as an engaged pair. His family dynamic has been really comforting for me and our relationship because we are taken seriously and our relationship feels scene as mature with his family. My house, on the other hand, feels quite literally the complete opposite.\\n\\nAnyways, this kind of got off track, but it feels important and supportive of my feelings towards my displacement of how I spend my free time at my house, because when Iâ€™m at his house and we are doing separate things at times, I feel more comfortable and inspired engaging in my interests. It feels easier at his house. Suddenly Iâ€™ll find myself feeling more relaxed and the ideas start flowing naturally into my head. And sometimes thatâ€™s something as simple as feeling motivated to watch one of my fav movies. But when Iâ€™m at my own house, that motivation just doesnâ€™t seem to exist anymore. So Iâ€™m finding as I spill my guts right now, maybe Iâ€™m finding my environment is the issue. I donâ€™t know. Iâ€™d love to know what people on the outside (aka, you) have to say about all this! ðŸ¤ª'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = df_ctrl[\"full_text\"].apply(count_first_person).nlargest(1).index\n",
    "df_ctrl.loc[idx, \"full_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I only keep relevant columns such as:\n",
    "\n",
    "- ID\n",
    "- full_text\n",
    "- label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"title\", \"selftext\", \"subreddit\", \"author\",\n",
    "    \"created_utc\", \"created_dt\", \"score\", \"num_comments\",\n",
    "    \"url\", \"link_flair_text\", \"source\", \"matched_keyword\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7908 entries, 0 to 7907\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         7908 non-null   object\n",
      " 1   full_text  7908 non-null   object\n",
      " 2   label      7908 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 185.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# lets take a look at the final structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, Split data into 80/20 and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6326\n",
      "Validation size: 1582\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,        # 20% validation\n",
    "    stratify=df[\"label\"], # mixing lavels\n",
    "    random_state=42      \n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in tokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2401 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 9\n",
      "Max length: 4401\n",
      "Mean length: 268.90756704980845\n",
      "\n",
      "Percentiles:\n",
      "50th: 181.0\n",
      "75th: 326.0\n",
      "90th: 549.0\n",
      "95th: 777.8499999999995\n",
      "99th: 1459.9599999999991\n",
      "\n",
      ">512 tokens: 718 out of 6264 (11.46%)\n"
     ]
    }
   ],
   "source": [
    "texts = train_df[\"full_text\"].tolist()\n",
    "\n",
    "enc = tokenizer(\n",
    "    texts,\n",
    "    padding=False,       # true lengths, no padding\n",
    "    truncation=False,    # don't cut, we want full length\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "\n",
    "lengths = [len(ids) for ids in enc[\"input_ids\"]]\n",
    "\n",
    "print(\"Min length:\", np.min(lengths))\n",
    "print(\"Max length:\", np.max(lengths))\n",
    "print(\"Mean length:\", np.mean(lengths))\n",
    "\n",
    "print(\"\\nPercentiles:\")\n",
    "for q in [50, 75, 90, 95, 99]:\n",
    "    print(f\"{q}th:\", np.percentile(lengths, q))\n",
    "\n",
    "# How many exceed BERT's 512-token limit?\n",
    "over_512 = sum(l > 512 for l in lengths)\n",
    "print(\"\\n>512 tokens:\", over_512, \"out of\", len(lengths),\n",
    "      f\"({over_512 / len(lengths) * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alright, so since 11% of the dataset is longer than the allowed tokens, we will chunk them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "num_labels = 2  # depression vs casual\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'full_text', 'label'],\n",
       "     num_rows: 6264\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'full_text', 'label'],\n",
       "     num_rows: 1567\n",
       " }))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from pd to huggingface df\n",
    "train_hf = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_hf   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "train_hf, val_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and chunk\n",
    "MAX_LENGTH = 512  \n",
    "\n",
    "def tokenize_and_chunk(examples):\n",
    "    enc = tokenizer(\n",
    "        examples[\"full_text\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,               # allow splitting into multiple chunks\n",
    "        padding=False,                 # padding will be done later by data collator\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    # Map each chunk back to its original example index\n",
    "    overflow_mapping = enc[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "    labels = []\n",
    "    sample_ids = []\n",
    "    for i in overflow_mapping:\n",
    "        labels.append(examples[\"label\"][i])\n",
    "        if \"id\" in examples:\n",
    "            sample_ids.append(examples[\"id\"][i])\n",
    "        else:\n",
    "            sample_ids.append(i)  # fallback index if no id column\n",
    "\n",
    "    enc[\"labels\"] = labels\n",
    "    enc[\"sample_id\"] = sample_ids\n",
    "\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fa56b53a5a489c886b257c2104ac2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383b87e12d954ef8b391c6992134b0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'labels', 'sample_id'],\n",
       "     num_rows: 7252\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'labels', 'sample_id'],\n",
       "     num_rows: 1805\n",
       " }))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply it to train and validation set\n",
    "train_chunked = train_hf.map(\n",
    "    tokenize_and_chunk,\n",
    "    batched=True,\n",
    "    remove_columns=train_hf.column_names,  \n",
    ")\n",
    "\n",
    "val_chunked = val_hf.map(\n",
    "    tokenize_and_chunk,\n",
    "    batched=True,\n",
    "    remove_columns=val_hf.column_names,\n",
    ")\n",
    "\n",
    "train_chunked, val_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handles padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    3206\n",
      "1    3058\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions:\n",
      "label\n",
      "0    0.511814\n",
      "1    0.488186\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_df[\"label\"].value_counts())\n",
    "print(\"\\nProportions:\")\n",
    "print(train_df[\"label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running experiment: bert_default_lr2e-5_ep3 =====\n",
      "lr=2e-05, epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_19609/3300655145.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/nadiahajighassem/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2721' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/2721 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 3. Run your three experiments (A, B, C)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m results_default = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbert_default_lr2e-5_ep3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m results_tuned_epochs = run_experiment(\n\u001b[32m     99\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mbert_tuned_lr2e-5_ep4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    100\u001b[39m     learning_rate=\u001b[32m2e-5\u001b[39m,\n\u001b[32m    101\u001b[39m     num_epochs=\u001b[32m4\u001b[39m,\n\u001b[32m    102\u001b[39m )\n\u001b[32m    104\u001b[39m results_tuned_lr = run_experiment(\n\u001b[32m    105\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mbert_tuned_lr3e-5_ep3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m     learning_rate=\u001b[32m3e-5\u001b[39m,\n\u001b[32m    107\u001b[39m     num_epochs=\u001b[32m3\u001b[39m,\n\u001b[32m    108\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(name, learning_rate, num_epochs)\u001b[39m\n\u001b[32m     60\u001b[39m trainer = Trainer(\n\u001b[32m     61\u001b[39m     model=model,\n\u001b[32m     62\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     68\u001b[39m )\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# ----- Train -----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# ----- Evaluate on validation set (one time, at the end) -----\u001b[39;00m\n\u001b[32m     74\u001b[39m metrics = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/accelerate/data_loader.py:579\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m next_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2880\u001b[39m, in \u001b[36mDataset.__getitems__\u001b[39m\u001b[34m(self, keys)\u001b[39m\n\u001b[32m   2878\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: \u001b[38;5;28mlist\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m   2879\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2880\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2881\u001b[39m     n_examples = \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[32m   2882\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch.items()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2876\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2874\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2875\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m-> \u001b[39m\u001b[32m2876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2858\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2856\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2857\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2858\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2861\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:658\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    656\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:415\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:471\u001b[39m, in \u001b[36mPythonFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lazy:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NLP-exam-2025-CogSci/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:150\u001b[39m, in \u001b[36mPythonArrowExtractor.extract_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# 1. Metrics (accuracy, precision, recall, F1)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "    }\n",
    "\n",
    "# 2. Helper: run one experiment + save model + metrics\n",
    "def run_experiment(name: str, learning_rate: float, num_epochs: int):\n",
    "    \"\"\"\n",
    "    Trains a fresh BERT model with given hyperparameters,\n",
    "    evaluates on val_chunked, saves:\n",
    "      - model in ./runs/{name}/best_model\n",
    "      - eval metrics in ./runs/{name}/eval_results_final.json\n",
    "    and returns the metrics dict.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== Running experiment: {name} =====\")\n",
    "    print(f\"lr={learning_rate}, epochs={num_epochs}\")\n",
    "\n",
    "    # fresh model for each run\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "    )\n",
    "\n",
    "    out_dir = Path(f\"./runs/{name}\")\n",
    "\n",
    "    # NOTE: only using arguments that exist in older Transformers versions\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(out_dir),\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        seed=42,\n",
    "        # no evaluation_strategy, save_strategy, load_best_model_at_end here\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_chunked,\n",
    "        eval_dataset=val_chunked,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # ----- Train -----\n",
    "    trainer.train()\n",
    "\n",
    "    # ----- Evaluate on validation set (one time, at the end) -----\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    print(metrics)\n",
    "\n",
    "    # ----- Ensure output dir exists -----\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ----- Save metrics to JSON -----\n",
    "    with open(out_dir / \"eval_results_final.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # ----- Save the final model (treat as 'best') -----\n",
    "    # This will create ./runs/{name}/best_model/ with config + weights + tokenizer\n",
    "    trainer.save_model(out_dir / \"best_model\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# 3. Run your three experiments (A, B, C)\n",
    "results_default = run_experiment(\n",
    "    name=\"bert_default_lr2e-5_ep3\",\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    ")\n",
    "\n",
    "results_tuned_epochs = run_experiment(\n",
    "    name=\"bert_tuned_lr2e-5_ep4\",\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=4,\n",
    ")\n",
    "\n",
    "results_tuned_lr = run_experiment(\n",
    "    name=\"bert_tuned_lr3e-5_ep3\",\n",
    "    learning_rate=3e-5,\n",
    "    num_epochs=3,\n",
    ")\n",
    "\n",
    "print(\"\\nSummary of results:\")\n",
    "print(\"Model A (2e-5, 3 ep):\", results_default)\n",
    "print(\"Model B (2e-5, 4 ep):\", results_tuned_epochs)\n",
    "print(\"Model C (3e-5, 3 ep):\", results_tuned_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING FOR OVERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7341/2599889993.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_a = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "RUN_NAME = \"bert_default_lr2e-5_ep3\"  # Model A\n",
    "\n",
    "model_path = Path(f\"./runs/{RUN_NAME}/best_model\")\n",
    "\n",
    "# Reload the trained model\n",
    "model_a = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Reuse the same data_collator and tokenizer you already have\n",
    "# data_collator, tokenizer, train_chunked, val_chunked assumed defined\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "    }\n",
    "\n",
    "# Dummy TrainingArguments (we're not training, just using Trainer for predict/evaluate)\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=f\"./runs/{RUN_NAME}/eval_again\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_a = Trainer(\n",
    "    model=model_a,\n",
    "    args=eval_args,\n",
    "    train_dataset=train_chunked,\n",
    "    eval_dataset=val_chunked,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics (Model A):\n",
      "  accuracy : 0.96398891966759\n",
      "  precision: 0.9650565262076053\n",
      "  recall   : 0.9680412371134021\n",
      "  f1       : 0.9665465774575399\n",
      "\n",
      "Validation confusion matrix (rows=true, cols=pred):\n",
      "[[801  34]\n",
      " [ 31 939]]\n"
     ]
    }
   ],
   "source": [
    "# Validation predictions\n",
    "val_pred = trainer_a.predict(val_chunked)\n",
    "y_val_true = val_pred.label_ids\n",
    "y_val_pred = val_pred.predictions.argmax(-1)\n",
    "\n",
    "print(\"Validation metrics (Model A):\")\n",
    "print(\"  accuracy :\", accuracy_score(y_val_true, y_val_pred))\n",
    "print(\"  precision:\", precision_score(y_val_true, y_val_pred))\n",
    "print(\"  recall   :\", recall_score(y_val_true, y_val_pred))\n",
    "print(\"  f1       :\", f1_score(y_val_true, y_val_pred))\n",
    "\n",
    "cm_val = confusion_matrix(y_val_true, y_val_pred)\n",
    "print(\"\\nValidation confusion matrix (rows=true, cols=pred):\")\n",
    "print(cm_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train predictions\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_pred = \u001b[43mtrainer_a\u001b[49m.predict(train_chunked)\n\u001b[32m      3\u001b[39m y_train_true = train_pred.label_ids\n\u001b[32m      4\u001b[39m y_train_pred = train_pred.predictions.argmax(-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer_a' is not defined"
     ]
    }
   ],
   "source": [
    "# Train predictions\n",
    "train_pred = trainer_a.predict(train_chunked)\n",
    "y_train_true = train_pred.label_ids\n",
    "y_train_pred = train_pred.predictions.argmax(-1)\n",
    "\n",
    "print(\"\\nTrain metrics (Model A):\")\n",
    "print(\"  accuracy :\", accuracy_score(y_train_true, y_train_pred))\n",
    "print(\"  precision:\", precision_score(y_train_true, y_train_pred))\n",
    "print(\"  recall   :\", recall_score(y_train_true, y_train_pred))\n",
    "print(\"  f1       :\", f1_score(y_train_true, y_train_pred))\n",
    "\n",
    "cm_train = confusion_matrix(y_train_true, y_train_pred)\n",
    "print(\"\\nTrain confusion matrix (rows=true, cols=pred):\")\n",
    "print(cm_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_chunked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     21\u001b[39m     preds = np.argmax(logits, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy_score(labels, preds),\n\u001b[32m     24\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: precision_score(labels, preds),\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: recall_score(labels, preds),\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: f1_score(labels, preds),\n\u001b[32m     27\u001b[39m     }\n\u001b[32m     29\u001b[39m trainer_c = Trainer(\n\u001b[32m     30\u001b[39m     model=model_c,\n\u001b[32m     31\u001b[39m     args=eval_args,\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     train_dataset=\u001b[43mtrain_chunked\u001b[49m,\n\u001b[32m     33\u001b[39m     eval_dataset=val_chunked,\n\u001b[32m     34\u001b[39m     data_collator=data_collator,\n\u001b[32m     35\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     36\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Validation metrics (you basically already have these, but for completeness)\u001b[39;00m\n\u001b[32m     40\u001b[39m val_pred = trainer_c.predict(val_chunked)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_chunked' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "RUN_NAME = \"bert_tuned_lr3e-5_ep3\"  # Model C's run name\n",
    "model_path = Path(f\"./runs/{RUN_NAME}/best_model\")\n",
    "\n",
    "model_c = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=f\"./runs/{RUN_NAME}/eval_again\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "    }\n",
    "\n",
    "trainer_c = Trainer(\n",
    "    model=model_c,\n",
    "    args=eval_args,\n",
    "    train_dataset=train_chunked,\n",
    "    eval_dataset=val_chunked,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Validation metrics (you basically already have these, but for completeness)\n",
    "val_pred = trainer_c.predict(val_chunked)\n",
    "y_val_true = val_pred.label_ids\n",
    "y_val_pred = val_pred.predictions.argmax(-1)\n",
    "\n",
    "print(\"Validation metrics (Model C):\")\n",
    "print(\"  accuracy :\", accuracy_score(y_val_true, y_val_pred))\n",
    "print(\"  precision:\", precision_score(y_val_true, y_val_pred))\n",
    "print(\"  recall   :\", recall_score(y_val_true, y_val_pred))\n",
    "print(\"  f1       :\", f1_score(y_val_true, y_val_pred))\n",
    "\n",
    "# Train metrics\n",
    "train_pred = trainer_c.predict(train_chunked)\n",
    "y_train_true = train_pred.label_ids\n",
    "y_train_pred = train_pred.predictions.argmax(-1)\n",
    "\n",
    "print(\"\\nTrain metrics (Model C):\")\n",
    "print(\"  accuracy :\", accuracy_score(y_train_true, y_train_pred))\n",
    "print(\"  precision:\", precision_score(y_train_true, y_train_pred))\n",
    "print(\"  recall   :\", recall_score(y_train_true, y_train_pred))\n",
    "print(\"  f1       :\", f1_score(y_train_true, y_train_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for finetuning BERT\n",
    "\n",
    "### The notebook is split up into the following sections:\n",
    "1. Clean data\n",
    "2. Split dataset into 80/20 split and tokenize\n",
    "3. Pipeline and finetuning BERT\n",
    "4. Evaluation metrics\n",
    "5. Creating tables and confusion matrix for report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# sklearn i\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# transformers \n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in dataset\n",
    "df = pd.read_csv(\"../Scraping/clean_reddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "CasualConversation    4044\n",
       "depression            3864\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets do a quick pronoun count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== DEPRESSION ==========\n",
      "Total pronoun counts: Counter({'i': 91360, 'my': 25372, 'me': 15784, 'myself': 4715, 'mine': 142})\n",
      "Total posts: 3864\n",
      "Total pronouns: 137373\n",
      "Average per post: 35.55201863354037\n",
      "Median per post: 26\n",
      "\n",
      "========== CASUALCONVERSATION ==========\n",
      "Total pronoun counts: Counter({'i': 28393, 'my': 8376, 'me': 4450, 'myself': 713, 'mine': 154})\n",
      "Total posts: 4044\n",
      "Total pronouns: 42086\n",
      "Average per post: 10.40702274975272\n",
      "Median per post: 7\n"
     ]
    }
   ],
   "source": [
    "# First-person singular pronouns\n",
    "FIRST_PERSON_PRONOUNS = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "\n",
    "# Count total number of first-person pronouns in a single post\n",
    "def count_first_person(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    return sum(1 for t in tokens if t in FIRST_PERSON_PRONOUNS)\n",
    "\n",
    "# Count each individual pronoun\n",
    "def count_each_pronoun(text):\n",
    "    if not isinstance(text, str):\n",
    "        return Counter()\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    return Counter(t for t in tokens if t in FIRST_PERSON_PRONOUNS)\n",
    "\n",
    "# split by subreddit\n",
    "df_dep = df[df[\"subreddit\"] == \"depression\"]\n",
    "df_ctrl = df[df[\"subreddit\"] == \"CasualConversation\"]\n",
    "\n",
    "# total and per-post counts \n",
    "def analyze_group(df_group, name):\n",
    "    total_counter = Counter()\n",
    "    per_post_counts = []\n",
    "\n",
    "    for text in df_group[\"full_text\"]:\n",
    "        per_post_counts.append(count_first_person(text))\n",
    "        total_counter.update(count_each_pronoun(text))\n",
    "\n",
    "    print(f\"\\n========== {name.upper()} ==========\")\n",
    "    print(\"Total pronoun counts:\", total_counter)\n",
    "    print(\"Total posts:\", len(df_group))\n",
    "    print(\"Total pronouns:\", sum(per_post_counts))\n",
    "    print(\"Average per post:\", sum(per_post_counts) / len(per_post_counts))\n",
    "    print(\"Median per post:\", sorted(per_post_counts)[len(per_post_counts)//2])\n",
    "\n",
    "# Run analysis for both groups\n",
    "analyze_group(df_dep, \"Depression\")\n",
    "analyze_group(df_ctrl, \"CasualConversation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the length of the posts from the dataset from r/depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/3604314048.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dep[\"char_len\"] = df_dep[\"full_text\"].str.len()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     3864.000000\n",
       "mean      1543.149845\n",
       "std       1548.748249\n",
       "min         43.000000\n",
       "25%        635.750000\n",
       "50%       1103.000000\n",
       "75%       1895.250000\n",
       "max      18017.000000\n",
       "Name: char_len, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dep[\"char_len\"] = df_dep[\"full_text\"].str.len()\n",
    "df_dep[\"char_len\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the length of the posts from the dataset from r/CasualConversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/3401160515.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ctrl[\"char_len\"] = df_ctrl[\"full_text\"].str.len()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    4044.000000\n",
       "mean      748.936944\n",
       "std       667.918083\n",
       "min        80.000000\n",
       "25%       347.000000\n",
       "50%       542.000000\n",
       "75%       915.000000\n",
       "max      8676.000000\n",
       "Name: char_len, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ctrl[\"char_len\"] = df_ctrl[\"full_text\"].str.len()\n",
    "df_ctrl[\"char_len\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the max length from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I\\'m 18 and my life is a mess of regret. I need to get this off my chest...\\n\\nI don\\'t even know where to begin. I\\'m writing this because my mental health is in the gutter and I need to just put it all out there. Maybe someone can make sense of it, because I can\\'t.\\n\\nI was born in April 2006 in Lansing, Michigan. I\\'m a guy. Ever since I was young, I have always been into girls. Like, when I was in elementary school I used to, oh god... this is embarrassing... I would pretend I dropped something like a pencil or an eraser look under the table I was sat at to see under the girls\\' skirts in my class. I know, it\\'s disgusting.\\n\\nI grew up and had a good childhood. I was a chubby kid back then. Now, when I started highschool, I used to have no discipline and no sense of hygiene. I didn\\'t take showersâ€” only sometimes - like once a week or once every two weeks.\\n\\nI woke up, went to school, and didn\\'t use the toilets in school since I hated them and thought they were disgusting. I held my pee in until I got home. I also held my poop in, which is what caused my body to always be warm and that\\'s what caused my skin to go bad and made it fill with pimples. My parents warned me not to pop any pimples as they can cause permanent scarring and damage, but I was an idiot and around 11 or 12 years old back then so I ignored them and thought they were just making a bunch of stuff up.\\n\\nMy usual routine was: wake up, eat breakfast, go to school, don\\'t pay attention in class (since I was too \"cool\" for it), eat lunch during lunchtime, come home and play on my PlayStation 4 for long hours or use my iPad to play games, etc. Yes, I was spoiled. Also, I was the first child, the oldest one in the house with one younger brother. I was living the dream.\\n\\nI always used to hold in my poop, which is very dangerous, since I didn\\'t wanna waste my time on the toilet. Sometimes I couldn\\'t hold it so I had to use the toilet, like when I was excited about something or if I ate too much. It\\'s already pretty bad, I know, but things are about to go haywire so... yeah.\\n\\nIt was here where I started to watch \"spicy\" content. It wasn\\'t porn; I didn\\'t fully discover it. I started to watch these kissing prank videos, nude celebrities, etc. Then I got caught by my parents, which was embarrassing.\\n\\nI was also very, VERY horny at this stage of my life. During this time I was also bullied quite a lot. I was always the tallest in my grade. I started to develop a little mustache (little). I was a pretty chubby kid and got bullied and got called \"fat shit\" in school. Girls made fun of me, basically everyone did.\\n\\nSo my dad decided to put me on the treadmill so I lose weight because this extra weight wasn\\'t good for my health and plus he didn\\'t want to see his son be lazy everyday. He pushed me every day until in a month\\'s time I could jog at a pace of 8km/h for 40 minutes. I started to lose weight during this period and could eat almost anything since I was still growing and had a pretty fast metabolism and was always hungry and felt really hot (fast metabolism). It was amazing; and I was starting to get in shape.\\n\\nBut that\\'s when things took a turn. I was so horny. I was watching kissing pranks and I didn\\'t realize they were fake. I took some pointers from those videos and went up to actual women... God, this is... I have no words to describe this abomination of an act.\\n\\nI talked to them seductively and had a boner in front of them. I saw this woman walking into the park and followed her and talked to her. She was very kind so I thought kindness would mean I could touch her, so I touched her boob. She quickly threw my hand away and said \"No,\" and I still kept on doing it. She said she\\'ll report me to the police. The next day that same lady comes and visits my house, with her husband and told my parents everything.\\n\\nMy parents were devastated. Their face went pale. My heart was pounding a million times per second. I had this bad feeling or sensation wash up on me. Her husband threatened my parents, said they\\'ll report this to the police, and was very pissed.\\n\\nMy father, a high moral and intelligent, high honour good man who never had to apologize or beg someone like this before, begged him not to report me to the police. He went down to his knees and put his head on his shoes and cried (I know this because my mom told me after all this). I was sitting in my room listening. My dad was very pissed and angry and disappointed. He had so many emotions; he couldn\\'t believe his son could do this. He warned me not to do it again.\\n\\nNow keep in mind, I\\'m dumbâ€”like stone-age level dumb. Like, really dumb, even though nothing is wrong with my brain. So during this time, watching porn etc., I was looking at women\\'s vaginas and wondering where the penis is. I thought women had penises. I was 12, mind you. I was looking for videos of women with penises. I didn\\'t even learn that women have anuses too until I was 15. See, I didn\\'t focus in school, didn\\'t understand English very well, had limited knowledge and nothing about maths or science. I was basically a fuck-up.\\n\\nNow in my family there is this no-privacy rule where our room (me and my brother slept in the same room) had to have the door open always. So I had not a lot of time to watch porn. I had wet dreams during these periods and wondered why my pants suddenly get filled up with this sticky white liquid when I wake up (cum), or why I eject this white liquid when I watch porn. I thought there was something wrong with me or why I have a boner. I didn\\'t know what these were; I was new to this. I genuinely thought there is something wrong with me. We don\\'t usually have sex talk in my family; they don\\'t talk about these things openly.\\n\\nDuring this time, age 12-13 (which most of the things that happened are around), I had so many school crushes on girls. I lusted over them heavily but didn\\'t dare touch them or anything because I knew they would complain to the headmaster\\'s office and parents will be involved, etc.\\n\\nBut it was summer a few months later and I looked at them in a lustful and creepy way. They were my age, all in my class. I looked at their butts and got super horny. It was sports day and I looked at their boobs; they were slim fit and had good body. So I looked at these two sisters who wore tight leggings or tights and I could see their butt and kept looking when no one was seeing and fantasized having sex with them. They had a ponytail and were beautiful.\\n\\nBut then even worse happens. I did the same act again and started to call on girls and be like \"hey baby nice tits or sexy ass wanna fuck\" etc., like some disgusting rapist shit, and looked them lustfully up and down and they could see it. I started harassing girls, following them, and some even said they\\'ll report me.\\n\\nSo one day, from afar from a window, I started following a girl and looked from the window to see where she was going. And that\\'s when my dad caught me. He took me home and hit me multiple times with a belt and was very angry with me. I wet my pants cause of the beating (which was well deserved; he should have broken my face).\\n\\nNow the reason for this extreme horniness is because I didn\\'t know how to jerk off or masturbate. Kids this age know, but not Mr. Dumbo here (me).\\n\\nSo after this fucked up situation, my dad banned me for 3 months from video games, iPad, or any screen, etc. He also shaved my head during this time since it was summer (and my brother\\'s head too, but my younger brother could adapt. He was also punished for my actionsâ€”collective punishment, I guess).\\n\\nI would play chess, study, or do maths, go outside, bring groceries, etc.\\n\\nAnd also remember that ever since I was a child, like 6 or 8 years old, my mum used to hit me sometimes when she made me study and I got things wrong. Since my dumb brain couldn\\'t comprehend maths or couldn\\'t concentrate or learn my times tables, etc., she used to hit me with hangers or rulers (the bendy ones that could bend that hurt, but not like full force, just punishment). I also have a grandpa and dad who would criticize or scold me sometimes if I made some mistakes. The men did that.\\n\\nI had a friend who would come to my house sometimes and he was kind of like my best friend in elementary school until I left. When I went to high school I had no friends; I was a loner. I wanted to be friends with someone but they weren\\'t really my friends. There were these two guys: top of the class and heavy into video games, etc. I asked them for some math advice and walked with them to the school library or canteen during lunch break. I used to play video games with them; they added me to PSN (PlayStation Network) and we played online.\\n\\nBut yeah, my dad said since I was banned forever until I improved my grades, he\\'ll unban me from everything until I improve. So I did. I worked very, very hard to understand math (my weakness; I was like a grade 3 person in grade 8 when it came to maths, and everything else).\\n\\nSo I watched basics all of them for maths: what are numbers, how to add and subtract and multiply and divide and work with fractions, decimals, PEMDAS (I didn\\'t even know division properly), then times tables. I worked very hard and started to stay late after school (a thing I never did since I thought only nerds do this, since I\\'m not a nerd). I stayed late after school and asked teachers about my maths misconceptions and any help they could give. It paid off and I started getting 60% in my examsâ€”a huge jump from 7-10%. I also worked hard in physics class and paid attention and got 80% marks too.\\n\\nAnd my dad also told me to read at least for one hour a day a book, whether it\\'s a fiction or non-fiction. So I started reading Harry Potter, Diary of a Wimpy Kid, etc., and improved my English a lot. I used my phone dictionary (I also had a phone; my mum\\'s iPhone 5 was given down to me) and it had parental lock and everything so I could not surf the web or watch YouTube, only dictionary access. So I looked up words and what they meant and worked very hard. My English reading grade went from a level of a 9-year-old to a level of a 12.5-year-old in a span of 3 months. I was 13 then.\\n\\nSo they called my dad and told him this. He was very happy and took unbanned me so I could watch TV, use my laptop (he gave me a laptop for personal use and for school). He said there is gonna be a limit to video games. Like before, when I played video games whenever or however much I could, this time it was only for Fridays (when I come home from school), Saturdays and Sundays for 2 hours each day. And he also said he expects me to do my homework and not delay it or miss any deadlines. So I was always punctual to school, never missed homework, and mostly did my homework during lunch in the school library. If there was more, I finished it at home in under an hour. I was a good kid back then.\\n\\nNow my dad and my grandpa make me do things like bring them a glass of water or help them with something, etc., like normal things. But they also heavily criticized me and called me dumb and sometimes make fun of my brain.\\n\\nFor example, my grandpa will ask, \"Hey, tighten this screw here,\" or \"bring me something from the toolbox.\" It was either I couldn\\'t tighten the screw or find the thing he wants from the toolbox. Simple things like that resulted in me getting scolded when the item, for example, I was looking for was in the toolbox.\\n\\nAnother example: I accidentally opened the door without looking when there was a person walking on the side and hit him. My dad had to apologize to the person (he is a very kind and peaceful guy) but then he angrily looked at me and cursed at me, \"son of a bitch, you didn\\'t see him coming!\" Sometimes it results in a mild slap (not too hard).\\n\\nHe cares deeply about respect and honour and doesn\\'t take lightly when someone disrespects the family name, the religion, the country, the race, etc. He says to me and my brother we represent all these things. My dad is a short-tempered guy but a good guy nonetheless.\\n\\nNow fast forward. I completed high school and got decent grades. But now I\\'m 18 and I am free. I\\'m looking for a job and got a local job. Now I\\'m thinking about sex and love and girls and women, etc.\\n\\nI was heavy on porn and by then I had already realized women don\\'t have penises but vaginas. In these porn videos I saw a \"happy ending\" massage being given to a guy by an Asian woman. So I decided to check Asian parlours near me that offer nuru massage or happy ending. I found one on Google Maps with good reviews (which don\\'t mention any sexual experiences) and visited it one day. I said \"happy ending.\"\\n\\nThere was this beautiful mid-30 or late early 40-year-old looking woman and she was hot. It was an experience like nothing I ever felt. I never kissed anyone well; I did that day. I never felt that women would be attracted to me and this was first time I was seeing someone attracted to me. Then after this day I tried to learn how to masturbate so I\\'m not horny any more so finally I discovered masturbation at age 18 (how dumb can you get to discover something that a 14 yr old or a 12 yr old should be discovering).\\n\\nI then regretted everything I did and felt guilty as heck. But I never got any attention from girls. After some time, she wasn\\'t there and left the massage parlour so didn\\'t work anymore. I used to go every 2 months to visit the massage parlour. I wanted affection. I wanted to be loved, to be cared for, and I wanted to be lusted over since I never experienced that before. That\\'s why I never forgot this experience.\\n\\nBut then after some time I started to get addicted. I saw other masseuses; all were Chinese masseuses since it was an oriental massage parlour. Eventually, I wished I didn\\'t. I had sex after 1 year of visiting it for the first time. Then after 3-4 months I had sex with escorts.\\n\\nI was devastated, empty, soulless that I could do such a thing. I was also masturbating this time but I tried to restrain myself but couldn\\'t. I wanted a physical person, a body I could touch and interact with who would willingly kiss me and have sex with me. So I kept visiting and drained my own money.\\n\\nI can never fully recover from this. I regret this deeply and have so, so, so much guilt. I wish I didn\\'t do this; this is not me. I wish I had found a good girl and made her my gf like everyone else does.\\n\\nAlso during this time I was learning to drive and still haven\\'t because you might say the problem is instructors or the teachers, but the whole problem is me. I had 4 instructors and still failed my driving test. On top of that, I had 60 hours of driving lessons. People pass in 30; by now I probably have 65 hours and I\\'m still not ready for driving. That\\'s how dumb I am.\\n\\nThe only thing I\\'m good at is running, which I can improve on and be better than most people since I\\'m above average.\\n\\nI wish I had friends to guide me, a gf, and some help. I\\'m alone. My parents are just parents, not friends. I love them but they\\'re not like friends; you don\\'t share everything with parents.\\n\\nI disappointed my family. I disappointed myself and the world. My dad once said \"You\\'re a burden to this world.\" I hate my life. I feel depressed.\\n\\nI can\\'t get a gf now. I\\'m done because I don\\'t like to lie. I would probably tell her all about me so she knows who she\\'s getting into because maybe she thought of me as someone else, but then the truth would probably hurt so better to let her know beforehand.\\n\\nAlso, I\\'m not a bad person internally. I wanna help people. I wanna give charity and probably create my own organization in the medical sector helping people for free, etc.\\n\\nThere\\'s way more to the story but I\\'m so tired of this. I\\'ll write the entire second part later, but here\\'s a summary of my life: I have no job, I\\'m not tall, average or slightly above average looking, average or slightly below average dick, not richâ€”I\\'m broke. I want help. Am I a bad person? I disappointed my parents a lot.\\n\\nI don\\'t understand things. Like the other day my dad told me to build this simple, super simple DIY science kit for 7+ ages that he ordered from Temu and I couldn\\'t figure out how to fix it. I tried fixing a DualShock 4 controller, a PS4 controller, but couldn\\'t. I\\'m a loser. I need help. I need advice.\\n\\nI don\\'t feel that guilty or disappointed in myself as I\\'m writing; I feel nothing. I feel empty, soulless. But some days when I think about or reflect on what I did in my life, I wish I had died and never been such an asshole. My mental health isn\\'t even strong right now.\\n\\nI\\'m working hard right now and trying my best to improve myself, learn new skills, become a better person etc. But I feel lonely. I don\\'t wanna die alone - that\\'s one of my greatest fear. But I guess after all I did, I deserve to die alone with no family, no kids, no one to take care of me and a fuck-up in the end.\\n\\nI also fell in love with a K-pop idol. And before you say it\\'s lust, it\\'s not. I rarely fall in love but when I do I fall hard. I fell in love with this idol because she\\'s so kind, nice, and beautiful. My type is someone who looks like they came from the skies like an angel (like a Greek goddess). She\\'s half-Korean, half-white Australian and she looks so beautiful. I wasn\\'t initially attracted to her when I first saw her, but as she did more Insta lives and she smiles and talks so kindly and is so nice, and she shares so many similar traits to me (not those I listed, not those asshole traits)â€”like her favourite color is the same as mine, her favourite movie same as mine, her personality is happy and jumping and sunshine and very uplifting, etc.\\n\\nI want her, yet I know I don\\'t deserve her. I am also starting to see my hair balding (not yet but I can see signs). I\\'m done for. No girl will like me. I feel like killing myself but I can\\'t. Then sometimes I don\\'t wanna die and feel like it\\'s my duty to educate people and help people like me never make the same mistakes I did. I\\'m a lost cause, I\\'m empty, I\\'m soulless...\\n\\nI\\'m a...disappointment.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = df_dep[\"full_text\"].apply(count_first_person).nlargest(1).index\n",
    "df_dep.loc[idx, \"full_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tips on how to find hobbies as a 22 year old female, plus me spilling other mumbo jumbo!\\n\\nHey yall, bit of a random post to make but it feels necessary and I need advice. I often feel stuck with my free time because I donâ€™t have any time consuming hobbies. Sure, I enjoy to be crafty and creative and do set aside time to do things I enjoy such as making jewelry, thrifting, doing my makeup routine every morning, drawing, baking/cooking, etc. Butâ€¦ this stuff doesnâ€™t isolate a large portion of my time everyday. Iâ€™m a very social motivated person, and my favorite thing to do is spend time with my finance and my friends. Itâ€™s hard because we donâ€™t live together yet, so the concept of being within each otherâ€™s company even whilst doing separate things isnâ€™t a reality yet. Aside from work, we spend almost all our free time together, which I absolutely adore. \\n\\nWhen I was younger, I really enjoyed video games. My fiancÃ© (21m) still loves to play video games in his free time, which makes me happy for him that he never lost the love for them like I did. However, I find it hard that I donâ€™t have anything in my life apart from him that I can get lost in for hours like he can with his video games. It feels like I donâ€™t know how to show up for myself in a way, because it used to be so easy to figure out what to do with my free time. Sure, I enjoy watching movies/shows when Iâ€™m in the mood, but this isnâ€™t typically something I can do for hours. I am the type of person that prefers to be out and about in her free time, whether that be driving around or visiting stores I like, or even going somewhere new. Itâ€™s all about the adventure for me. And whatâ€™s great is my fiancÃ© and I love to go fishing together, which is something weâ€™ve been really into the last couple months and has been very consistent in our lives. Itâ€™s an amazing hobby weâ€™ve gotten into together. So fishing is the most time consuming hobby Iâ€™ve discovered and been able to get into as my 22 year old self, but again this is something I enjoy doing WITH my fiancÃ©. Pretty much, my goal here is to get advice on how I can enjoy my alone time better, and find things to do that I genuinely can get lost in for hours without necessarily needing company. Even if I was more into gaming, Iâ€™d prefer to play with people, which is what my fiancÃ© does with his friends. So maybe hobbies are better shared with people? Iâ€™m not sure if me striving for hobbies I can do by myself to feel more whole is realistic or not, but Iâ€™m curious if thereâ€™s other people out there in the same boat as me. Because whenever Iâ€™m alone I rather be with people (fiancÃ©, friends, family). It feels more like a preference than anything, but I wish I could find the free time just for myself, me spending time with me and only me. Donâ€™t get me wrong, people need people, so my goal isnâ€™t to shut people out. But itâ€™s hard because part of the reason I feel like itâ€™s difficult for me to feel more inspired and motivated to find things to do is due to my environment. I still live at home, and my sister is either at work or doing something with her fiance, so we donâ€™t have a lot of time together anymore. And my mom is someone Iâ€™m not super close to and have tried to involve her more in my life but she never reciprocated that need. And my dad and I are super close, but heâ€™s someone who will be off for a selected period of time and then off working as an assistant director, so our time together is inconsistent. Maybe itâ€™s also because Iâ€™m getting older, but my goals are aligned with more grown up things. Like I want my own place with my fiance, him and I are starting nursing next week, I obviously adore to have a family of my own, and all my mind can focus on is these things. I donâ€™t really make time for ME anymore, in a weird way I feel. But something I love about his house is that thereâ€™s so much life to it. Thereâ€™s animals, and my fiancÃ©â€™s niece is over a lot, and overall his family is very respectful of our relationship, so I prefer spending time there as opposed to my house. My parents are old school so thereâ€™s weird restrictions about where we can hang at my house (like we canâ€™t go in my room, even with the door open my parents are too nosy), and overall my parents are very much helicopter parents, and I donâ€™t feel they take my relationship that seriously so it feels weird being around them with my fiance, which sucks. But at my fiancÃ©s house I get the love from my future mother in law that I never got with my mom, his family has a cat which is my favorite pet to have and my cat died a few years ago. My house just feels empty and lonely compared to his. And this feels relevant to my story because sure, my room has grown to be my safe space in my house, but something about my fiancÃ©s house feels safer overall, which I think gives my fiance a sense of comfort to be able to indulge in his interests better. I mean, us being able to spend time together alone at his house in his room is a huge part of why our relationship feels more real and comfortable as opposed to my house, because thereâ€™s a sense of welcoming and trust from his family about me and our relationship as an engaged pair. His family dynamic has been really comforting for me and our relationship because we are taken seriously and our relationship feels scene as mature with his family. My house, on the other hand, feels quite literally the complete opposite.\\n\\nAnyways, this kind of got off track, but it feels important and supportive of my feelings towards my displacement of how I spend my free time at my house, because when Iâ€™m at his house and we are doing separate things at times, I feel more comfortable and inspired engaging in my interests. It feels easier at his house. Suddenly Iâ€™ll find myself feeling more relaxed and the ideas start flowing naturally into my head. And sometimes thatâ€™s something as simple as feeling motivated to watch one of my fav movies. But when Iâ€™m at my own house, that motivation just doesnâ€™t seem to exist anymore. So Iâ€™m finding as I spill my guts right now, maybe Iâ€™m finding my environment is the issue. I donâ€™t know. Iâ€™d love to know what people on the outside (aka, you) have to say about all this! ðŸ¤ª'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = df_ctrl[\"full_text\"].apply(count_first_person).nlargest(1).index\n",
    "df_ctrl.loc[idx, \"full_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I only keep relevant columns such as:\n",
    "\n",
    "- ID\n",
    "- full_text\n",
    "- label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"title\", \"selftext\", \"subreddit\", \"author\",\n",
    "    \"created_utc\", \"created_dt\", \"score\", \"num_comments\",\n",
    "    \"url\", \"link_flair_text\", \"source\", \"matched_keyword\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7908 entries, 0 to 7907\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         7908 non-null   object\n",
      " 1   full_text  7908 non-null   object\n",
      " 2   label      7908 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 185.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# lets take a look at the final structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split data into 80/20 and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6326\n",
      "Validation size: 1582\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,        # 20% validation\n",
    "    stratify=df[\"label\"], # mixing lavels\n",
    "    random_state=42      \n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in tokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 9\n",
      "Max length: 4401\n",
      "Mean length: 270.08330698703764\n",
      "\n",
      "Percentiles:\n",
      "50th: 181.0\n",
      "75th: 325.75\n",
      "90th: 549.0\n",
      "95th: 770.0\n",
      "99th: 1530.25\n",
      "\n",
      ">512 tokens: 732 out of 6326 (11.57%)\n"
     ]
    }
   ],
   "source": [
    "texts = train_df[\"full_text\"].tolist()\n",
    "\n",
    "enc = tokenizer(\n",
    "    texts,\n",
    "    padding=False,       # true lengths, no padding\n",
    "    truncation=False,    # don't cut, we want full length\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "\n",
    "lengths = [len(ids) for ids in enc[\"input_ids\"]]\n",
    "\n",
    "print(\"Min length:\", np.min(lengths))\n",
    "print(\"Max length:\", np.max(lengths))\n",
    "print(\"Mean length:\", np.mean(lengths))\n",
    "\n",
    "print(\"\\nPercentiles:\")\n",
    "for q in [50, 75, 90, 95, 99]:\n",
    "    print(f\"{q}th:\", np.percentile(lengths, q))\n",
    "\n",
    "# How many exceed BERT's 512-token limit?\n",
    "over_512 = sum(l > 512 for l in lengths)\n",
    "print(\"\\n>512 tokens:\", over_512, \"out of\", len(lengths),\n",
    "      f\"({over_512 / len(lengths) * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alright, so since 11% of the dataset is longer than the allowed tokens, we will chunk them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "num_labels = 2  # depression vs casual\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'full_text', 'label'],\n",
       "     num_rows: 6326\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'full_text', 'label'],\n",
       "     num_rows: 1582\n",
       " }))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from pd to huggingface df\n",
    "train_hf = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_hf   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "train_hf, val_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and chunk\n",
    "MAX_LENGTH = 512  \n",
    "\n",
    "def tokenize_and_chunk(examples):\n",
    "    enc = tokenizer(\n",
    "        examples[\"full_text\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,               # allow splitting into multiple chunks\n",
    "        padding=False,                 # padding will be done later by data collator\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    # Map each chunk back to its original example index\n",
    "    overflow_mapping = enc[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "    labels = []\n",
    "    sample_ids = []\n",
    "    for i in overflow_mapping:\n",
    "        labels.append(examples[\"label\"][i])\n",
    "        if \"id\" in examples:\n",
    "            sample_ids.append(examples[\"id\"][i])\n",
    "        else:\n",
    "            sample_ids.append(i)  # fallback index if no id column\n",
    "\n",
    "    enc[\"labels\"] = labels\n",
    "    enc[\"sample_id\"] = sample_ids\n",
    "\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d6871276b744f48224ceb03284e9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc286f82d6174ca4a810860f7027e89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'labels', 'sample_id'],\n",
       "     num_rows: 7342\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'labels', 'sample_id'],\n",
       "     num_rows: 1837\n",
       " }))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alright now lets apply it to train and validation set\n",
    "train_chunked = train_hf.map(\n",
    "    tokenize_and_chunk,\n",
    "    batched=True,\n",
    "    remove_columns=train_hf.column_names,  \n",
    ")\n",
    "\n",
    "val_chunked = val_hf.map(\n",
    "    tokenize_and_chunk,\n",
    "    batched=True,\n",
    "    remove_columns=val_hf.column_names,\n",
    ")\n",
    "\n",
    "train_chunked, val_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    3235\n",
      "1    3091\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportions:\n",
      "label\n",
      "0    0.511382\n",
      "1    0.488618\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Lets take a look at the data\n",
    "print(train_df[\"label\"].value_counts())\n",
    "print(\"\\nProportions:\")\n",
    "print(train_df[\"label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pipeline and finetuning (Note: This chunk takes multiple hours to run)\n",
    "\n",
    "I am fine-tuning three models with different hyperparameters to determine which one achieves the highest accuracy. The details for each model are as follows:\n",
    "\n",
    "- **Model A**:\n",
    "  - Learning Rate: `2e-5`\n",
    "  - Epochs: `3`\n",
    "\n",
    "- **Model B**:\n",
    "  - Learning Rate: `2e-5`\n",
    "  - Epochs: `4`\n",
    "\n",
    "- **Model C**:\n",
    "  - Learning Rate: `3e-5`\n",
    "  - Epochs: `3`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# these are metrics i want look at later (accuracy, precision, recall, F1)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# 2. Run one experiment + save model + metrics\n",
    "def run_experiment(name: str, learning_rate: float, num_epochs: int):\n",
    "    \"\"\"\n",
    "    Trains a fresh BERT model with given hyperparameters,\n",
    "    evaluates on val_chunked, saves:\n",
    "      - model in ./runs/{name}/best_model\n",
    "      - eval metrics in ./runs/{name}/eval_results_final.json\n",
    "    and returns the metrics dict.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== Running experiment: {name} =====\")\n",
    "    print(f\"lr={learning_rate}, epochs={num_epochs}\")\n",
    "\n",
    "    # fresh model for each run\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "    )\n",
    "\n",
    "    out_dir = Path(f\"./runs/{name}\")\n",
    "\n",
    "    # 3. defining training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(out_dir),\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        seed=42,\n",
    "        # load_best_model_at_end here\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_chunked,\n",
    "        eval_dataset=val_chunked,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on validation set (one time, at the end) -\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    print(metrics)\n",
    "\n",
    "    # lets check output dir exists \n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save metrics to JSON \n",
    "    with open(out_dir / \"eval_results_final.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Save the final model (treat as 'best')\n",
    "    # This will create ./runs/{name}/best_model/ with config + weights + tokenizer\n",
    "    trainer.save_model(out_dir / \"best_model\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# 4. Run the three models (A, B, C)\n",
    "results_default = run_experiment(\n",
    "    name=\"bert_default_lr2e-5_ep3\",\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    ")\n",
    "\n",
    "results_tuned_epochs = run_experiment(\n",
    "    name=\"bert_tuned_lr2e-5_ep4\",\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=4,\n",
    ")\n",
    "\n",
    "results_tuned_lr = run_experiment(\n",
    "    name=\"bert_tuned_lr3e-5_ep3\",\n",
    "    learning_rate=3e-5,\n",
    "    num_epochs=3,\n",
    ")\n",
    "\n",
    "print(\"\\nSummary of results:\")\n",
    "print(\"Model A (2e-5, 3 ep):\", results_default)\n",
    "print(\"Model B (2e-5, 4 ep):\", results_tuned_epochs)\n",
    "print(\"Model C (3e-5, 3 ep):\", results_tuned_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation metrics\n",
    "\n",
    "In this section, I evaluate the performance of the models to assess potential overfitting. I compute several metrics, including accuracy, precision, recall, and F1 score, as well as confusion matrices for both the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/340426016.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model A (bert_default_lr2e-5_ep3) ===\n",
      "Validation:\n",
      "  accuracy : 0.985302\n",
      "  precision: 0.983838\n",
      "  recall   : 0.988832\n",
      "  f1       : 0.986329\n",
      "  CM (rows=true, cols=pred):\n",
      "[[836  16]\n",
      " [ 11 974]]\n",
      "Train:\n",
      "  accuracy : 0.989785\n",
      "  precision: 0.988494\n",
      "  recall   : 0.992715\n",
      "  f1       : 0.990600\n",
      "  CM (rows=true, cols=pred):\n",
      "[[3315   46]\n",
      " [  29 3952]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/340426016.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model B (bert_tuned_lr2e-5_ep4) ===\n",
      "Validation:\n",
      "  accuracy : 0.990201\n",
      "  precision: 0.990863\n",
      "  recall   : 0.990863\n",
      "  f1       : 0.990863\n",
      "  CM (rows=true, cols=pred):\n",
      "[[843   9]\n",
      " [  9 976]]\n",
      "Train:\n",
      "  accuracy : 0.992509\n",
      "  precision: 0.992721\n",
      "  recall   : 0.993469\n",
      "  f1       : 0.993095\n",
      "  CM (rows=true, cols=pred):\n",
      "[[3332   29]\n",
      " [  26 3955]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/340426016.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model C (bert_tuned_lr3e-5_ep3) ===\n",
      "Validation:\n",
      "  accuracy : 0.989657\n",
      "  precision: 0.989858\n",
      "  recall   : 0.990863\n",
      "  f1       : 0.990360\n",
      "  CM (rows=true, cols=pred):\n",
      "[[842  10]\n",
      " [  9 976]]\n",
      "Train:\n",
      "  accuracy : 0.992509\n",
      "  precision: 0.992968\n",
      "  recall   : 0.993218\n",
      "  f1       : 0.993093\n",
      "  CM (rows=true, cols=pred):\n",
      "[[3333   28]\n",
      " [  27 3954]]\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "RUNS = {\n",
    "    \"Model A\": \"bert_default_lr2e-5_ep3\",     \n",
    "    \"Model B\": \"bert_tuned_lr2e-5_ep4\",        \n",
    "    \"Model C\": \"bert_tuned_lr3e-5_ep3\",       \n",
    "}\n",
    "\n",
    "def eval_one_model(model_label: str, run_name: str):\n",
    "    model_path = Path(f\"./runs/{run_name}/best_model\")\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing model path: {model_path}\")\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=f\"./runs/{run_name}/eval_again\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        do_eval=True,\n",
    "        do_predict=True,\n",
    "        seed=42,\n",
    "        report_to=[],  \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        train_dataset=train_chunked,\n",
    "        eval_dataset=val_chunked,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    def score_split(name, ds):\n",
    "        pred = trainer.predict(ds)\n",
    "        y_true = pred.label_ids\n",
    "        y_pred = pred.predictions.argmax(-1)\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred),\n",
    "            \"recall\": recall_score(y_true, y_pred),\n",
    "            \"f1\": f1_score(y_true, y_pred),\n",
    "        }\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        return metrics, cm\n",
    "\n",
    "    val_metrics, val_cm = score_split(\"val\", val_chunked)\n",
    "    train_metrics, train_cm = score_split(\"train\", train_chunked)\n",
    "\n",
    "    print(f\"\\n=== {model_label} ({run_name}) ===\")\n",
    "    print(\"Validation:\")\n",
    "    print(f\"  accuracy : {val_metrics['accuracy']:.6f}\")\n",
    "    print(f\"  precision: {val_metrics['precision']:.6f}\")\n",
    "    print(f\"  recall   : {val_metrics['recall']:.6f}\")\n",
    "    print(f\"  f1       : {val_metrics['f1']:.6f}\")\n",
    "    print(f\"  CM (rows=true, cols=pred):\\n{val_cm}\")\n",
    "\n",
    "    print(\"Train:\")\n",
    "    print(f\"  accuracy : {train_metrics['accuracy']:.6f}\")\n",
    "    print(f\"  precision: {train_metrics['precision']:.6f}\")\n",
    "    print(f\"  recall   : {train_metrics['recall']:.6f}\")\n",
    "    print(f\"  f1       : {train_metrics['f1']:.6f}\")\n",
    "    print(f\"  CM (rows=true, cols=pred):\\n{train_cm}\")\n",
    "\n",
    "# ---- run all ----\n",
    "for label, run_name in RUNS.items():\n",
    "    eval_one_model(label, run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The metrics still look suspuciously high, lets see if the accruacy is as high for post level metrics and not chunking metrics for the model B (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: runs/bert_tuned_lr2e-5_ep4/best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/3457465981.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk-level metrics: {'chunk_accuracy': 0.9902014153511159, 'chunk_precision': 0.9908629441624366, 'chunk_recall': 0.9908629441624366, 'chunk_f1': 0.9908629441624366}\n",
      "\n",
      "Post-level metrics: {'post_accuracy': 0.9905183312262958, 'post_precision': 0.9884020618556701, 'post_recall': 0.9922380336351876, 'post_f1': 0.9903163331181407}\n",
      "\n",
      "Post-level confusion matrix (rows=true, cols=pred):\n",
      "[[800   9]\n",
      " [  6 767]]\n"
     ]
    }
   ],
   "source": [
    "# really ensuring the same metric is computed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "\n",
    "# 1) model B\n",
    "\n",
    "RUN_NAME = \"bert_tuned_lr2e-5_ep4\"   \n",
    "run_dir = Path(f\"./runs/{RUN_NAME}\")\n",
    "\n",
    "best_model_dir = run_dir / \"best_model\"\n",
    "\n",
    "# If best_model doesn't exist, fall back to latest checkpoint in the run folder\n",
    "if best_model_dir.exists():\n",
    "    model_dir = best_model_dir\n",
    "else:\n",
    "    ckpts = sorted(\n",
    "        run_dir.glob(\"checkpoint-*\"),\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    if len(ckpts) == 0:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Couldn't find a saved model for RUN_NAME='{RUN_NAME}'. \"\n",
    "            f\"Looked for {best_model_dir} and {run_dir}/checkpoint-*. \"\n",
    "            \"If you no longer have the runs/ folder, youâ€™ll need to retrain or restore the checkpoint.\"\n",
    "        )\n",
    "    model_dir = ckpts[-1]\n",
    "\n",
    "print(\"Loading model from:\", model_dir)\n",
    "\n",
    "# 2) Reload model + tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "model.eval()  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "\n",
    "# 3) Build a Trainer for prediction\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_eval\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_train=False,\n",
    "    do_eval=False,\n",
    "    report_to=[],\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=val_chunked,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 4) Chunk-level predictions\n",
    "pred = trainer.predict(val_chunked)\n",
    "logits = pred.predictions\n",
    "chunk_labels = pred.label_ids\n",
    "chunk_preds = np.argmax(logits, axis=1)\n",
    "\n",
    "chunk_metrics = {\n",
    "    \"chunk_accuracy\": accuracy_score(chunk_labels, chunk_preds),\n",
    "    \"chunk_precision\": precision_score(chunk_labels, chunk_preds),\n",
    "    \"chunk_recall\": recall_score(chunk_labels, chunk_preds),\n",
    "    \"chunk_f1\": f1_score(chunk_labels, chunk_preds),\n",
    "}\n",
    "print(\"\\nChunk-level metrics:\", chunk_metrics)\n",
    "\n",
    "# 4) Post-level aggregation via sample_id\n",
    "\n",
    "sample_ids = np.array(val_chunked[\"sample_id\"])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"sample_id\": sample_ids,\n",
    "    \"label\": chunk_labels,\n",
    "    \"logit0\": logits[:, 0],\n",
    "    \"logit1\": logits[:, 1],\n",
    "})\n",
    "\n",
    "label_nunique = df.groupby(\"sample_id\")[\"label\"].nunique()\n",
    "if (label_nunique > 1).any():\n",
    "    bad = label_nunique[label_nunique > 1].index[:10].tolist()\n",
    "    print(\"WARNING: Some sample_ids have mixed labels (showing up to 10):\", bad)\n",
    "\n",
    "post = df.groupby(\"sample_id\", sort=False).agg(\n",
    "    label=(\"label\", \"first\"),\n",
    "    logit0=(\"logit0\", \"mean\"),\n",
    "    logit1=(\"logit1\", \"mean\"),\n",
    ")\n",
    "\n",
    "post_logits = post[[\"logit0\", \"logit1\"]].to_numpy()\n",
    "post_labels = post[\"label\"].to_numpy()\n",
    "post_preds = np.argmax(post_logits, axis=1)\n",
    "\n",
    "post_metrics = {\n",
    "    \"post_accuracy\": accuracy_score(post_labels, post_preds),\n",
    "    \"post_precision\": precision_score(post_labels, post_preds),\n",
    "    \"post_recall\": recall_score(post_labels, post_preds),\n",
    "    \"post_f1\": f1_score(post_labels, post_preds),\n",
    "}\n",
    "print(\"\\nPost-level metrics:\", post_metrics)\n",
    "\n",
    "print(\"\\nPost-level confusion matrix (rows=true, cols=pred):\")\n",
    "print(confusion_matrix(post_labels, post_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9925088531735222\n",
      "Train F1: 0.9930947897049592\n"
     ]
    }
   ],
   "source": [
    "train_pred = trainer.predict(train_chunked)\n",
    "train_logits = train_pred.predictions\n",
    "train_y = train_pred.label_ids\n",
    "train_yhat = np.argmax(train_logits, axis=1)\n",
    "\n",
    "train_acc = accuracy_score(train_y, train_yhat)\n",
    "train_f1  = f1_score(train_y, train_yhat)\n",
    "\n",
    "print(\"Train accuracy:\", train_acc)\n",
    "print(\"Train F1:\", train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tables and Confusion matrix for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/2845926242.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/2845926242.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/lvd540x91w94plzyvtdb9rpw0000gn/T/ipykernel_45069/2845926242.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/nadiahajighassem/Documents/GitHub/NLP_2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model by validation F1: Model B\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>LR</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Val Accuracy</th>\n",
       "      <th>Val Precision</th>\n",
       "      <th>Val Recall</th>\n",
       "      <th>Val F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model A</td>\n",
       "      <td>2e-5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model B</td>\n",
       "      <td>2e-5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model C</td>\n",
       "      <td>3e-5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model    LR  Epochs Val Accuracy Val Precision Val Recall Val F1\n",
       "0  Model A  2e-5       3        0.985         0.984      0.989  0.986\n",
       "1  Model B  2e-5       4        0.990         0.991      0.991  0.991\n",
       "2  Model C  3e-5       3        0.990         0.990      0.991  0.990"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/nadiahajighassem/Documents/GitHub/NLP_2025/results/model_metrics_table.md\n",
      "Saved: /Users/nadiahajighassem/Documents/GitHub/NLP_2025/results/model_metrics_table.tex\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 560x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/nadiahajighassem/Documents/GitHub/NLP_2025/results/figures/confusion_matrix_ModelB_val.png\n",
      "Saved: /Users/nadiahajighassem/Documents/GitHub/NLP_2025/results/figures/confusion_matrix_ModelB_val.pdf\n"
     ]
    }
   ],
   "source": [
    "# all models\n",
    "RUNS = {\n",
    "    \"Model A\": {\"run_name\": \"bert_default_lr2e-5_ep3\", \"lr\": 2e-5, \"epochs\": 3},\n",
    "    \"Model B\": {\"run_name\": \"bert_tuned_lr2e-5_ep4\",   \"lr\": 2e-5, \"epochs\": 4},\n",
    "    \"Model C\": {\"run_name\": \"bert_tuned_lr3e-5_ep3\",   \"lr\": 3e-5, \"epochs\": 3},\n",
    "}\n",
    "\n",
    "INCLUDE_TRAIN = False  \n",
    "OUT_DIR = Path(\"/Users/nadiahajighassem/Documents/GitHub/NLP_2025/results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# METRICS\n",
    "def compute_basic_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def eval_one_model(model_label: str, run_name: str):\n",
    "    model_path = Path(f\"./runs/{run_name}/best_model\")\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing model path: {model_path}\")\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=str(OUT_DIR / f\"tmp_eval_{run_name}\"),\n",
    "        per_device_eval_batch_size=8,\n",
    "        do_predict=True,\n",
    "        seed=42,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_pred = trainer.predict(val_chunked)\n",
    "    y_val_true = val_pred.label_ids\n",
    "    y_val_pred = val_pred.predictions.argmax(-1)\n",
    "    val_metrics = compute_basic_metrics(y_val_true, y_val_pred)\n",
    "    val_cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "\n",
    "    # Train \n",
    "    train_metrics, train_cm = None, None\n",
    "    if INCLUDE_TRAIN:\n",
    "        tr_pred = trainer.predict(train_chunked)\n",
    "        y_tr_true = tr_pred.label_ids\n",
    "        y_tr_pred = tr_pred.predictions.argmax(-1)\n",
    "        train_metrics = compute_basic_metrics(y_tr_true, y_tr_pred)\n",
    "        train_cm = confusion_matrix(y_tr_true, y_tr_pred)\n",
    "\n",
    "    return val_metrics, val_cm, train_metrics, train_cm\n",
    "\n",
    "\n",
    "# RUN EVALS + BUILD TABLE\n",
    "rows = []\n",
    "cms_val = {}   \n",
    "cms_train = {} \n",
    "\n",
    "for label, meta in RUNS.items():\n",
    "    run_name = meta[\"run_name\"]\n",
    "    lr = meta[\"lr\"]\n",
    "    epochs = meta[\"epochs\"]\n",
    "\n",
    "    val_metrics, val_cm, train_metrics, train_cm = eval_one_model(label, run_name)\n",
    "\n",
    "    cms_val[label] = val_cm\n",
    "    if INCLUDE_TRAIN and train_cm is not None:\n",
    "        cms_train[label] = train_cm\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": label,\n",
    "        \"LR\": f\"{lr:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\"),  \n",
    "        \"Epochs\": epochs,\n",
    "        \"Val Accuracy\": val_metrics[\"accuracy\"],\n",
    "        \"Val Precision\": val_metrics[\"precision\"],\n",
    "        \"Val Recall\": val_metrics[\"recall\"],\n",
    "        \"Val F1\": val_metrics[\"f1\"],\n",
    "        **({\n",
    "            \"Train Accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"Train Precision\": train_metrics[\"precision\"],\n",
    "            \"Train Recall\": train_metrics[\"recall\"],\n",
    "            \"Train F1\": train_metrics[\"f1\"],\n",
    "        } if INCLUDE_TRAIN and train_metrics is not None else {})\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Choose best model by Val F1\n",
    "best_row = df.iloc[df[\"Val F1\"].idxmax()]\n",
    "best_model = best_row[\"Model\"]\n",
    "best_cm = cms_val[best_model]\n",
    "\n",
    "# Pretty formatting for display/export\n",
    "df_pretty = df.copy()\n",
    "for c in df_pretty.columns:\n",
    "    if \"Val \" in c or \"Train \" in c:\n",
    "        df_pretty[c] = df_pretty[c].map(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "print(\"Best model by validation F1:\", best_model)\n",
    "display(df_pretty)\n",
    "\n",
    "# export\n",
    "md_metrics_path = OUT_DIR / \"model_metrics_table.md\"\n",
    "tex_metrics_path = OUT_DIR / \"model_metrics_table.tex\"\n",
    "\n",
    "md_text = df_pretty.to_markdown(index=False)\n",
    "tex_text = df_pretty.to_latex(index=False, escape=False)\n",
    "\n",
    "md_metrics_path.write_text(md_text, encoding=\"utf-8\")\n",
    "tex_metrics_path.write_text(tex_text, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\", md_metrics_path)\n",
    "print(\"Saved:\", tex_metrics_path)\n",
    "\n",
    "\n",
    "# Confusion Matrix with red and hreen\n",
    "\n",
    "# Color assignment\n",
    "colors = np.ones((2, 2, 4), dtype=float)  \n",
    "\n",
    "# Soft colors (readable with black text)\n",
    "light_green = np.array([0.78, 0.93, 0.78, 1.0])  \n",
    "light_red   = np.array([0.98, 0.82, 0.82, 1.0])  \n",
    "light_neutral = np.array([0.95, 0.95, 0.95, 1.0])  \n",
    "\n",
    "# Color assignment\n",
    "colors[1, 1] = light_green   # TP (true=1, pred=1)\n",
    "colors[0, 1] = light_red     # FP (true=0, pred=1)\n",
    "colors[1, 0] = light_red     # FN (true=1, pred=0)\n",
    "colors[0, 0] = light_green   # TN (true=0, pred=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.6, 4.4))\n",
    "ax.imshow(colors, interpolation=\"nearest\")\n",
    "\n",
    "# Add counts on top\n",
    "for (i, j), v in np.ndenumerate(best_cm):\n",
    "    ax.text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=12, color=\"black\")\n",
    "\n",
    "# Axis labels (make 0/1 meaning explicit)\n",
    "ax.set_xticks([0, 1], labels=[\"Pred 0\", \"Pred 1\"])\n",
    "ax.set_yticks([0, 1], labels=[\"True 0\", \"True 1\"])\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "ax.set_title(f\"Confusion matrix ({best_model}, validation)\")\n",
    "\n",
    "# Light gridlines to separate cells\n",
    "ax.set_xticks(np.arange(-.5, 2, 1), minor=True)\n",
    "ax.set_yticks(np.arange(-.5, 2, 1), minor=True)\n",
    "ax.grid(which=\"minor\", linestyle=\"-\", linewidth=1)\n",
    "ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "# Legend\n",
    "legend_items = [\n",
    "    mpatches.Patch(color=light_green, label=\"TP (Depression â†’ Depression)\"),\n",
    "    mpatches.Patch(color=light_red, label=\"FP/FN (False)\"),\n",
    "    mpatches.Patch(color=light_green, label=\"TN (Control â†’ Control)\"),\n",
    "]\n",
    "ax.legend(handles=legend_items, loc=\"upper center\", bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as PNG and PDF\n",
    "png_path = FIG_DIR / f\"confusion_matrix_{best_model.replace(' ', '')}_val.png\"\n",
    "pdf_path = FIG_DIR / f\"confusion_matrix_{best_model.replace(' ', '')}_val.pdf\"\n",
    "plt.savefig(png_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(pdf_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved:\", png_path)\n",
    "print(\"Saved:\", pdf_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

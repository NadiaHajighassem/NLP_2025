{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for scraping reddit posts\n",
    "\n",
    "### The notebook is split up into the following sections:\n",
    "1. Defining API\n",
    "2. Scrape posts from r/depression\n",
    "3. Scrape posts from r/Casual conversation\n",
    "4. Merge datasets, label them and remove possible duplications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# The weird letters are the cresidentials from my API\n",
    "\n",
    "reddit = praw.Reddit(client_id ='hQafbgnPnl6Jl_wabPofuA',\n",
    "                     client_secret ='OZ0EsYse7J-JiYE7kPk3mddQgMZ2nA',\n",
    "                     user_agent ='WebScraper/Own-Biscotti6249')\n",
    "                     \n",
    "# to verify whether the instance is authorized instance or not\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrape posts from r/depression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function to scrape 1000 posts from r/depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target number of posts to collect\n",
    "TARGET_COUNT = 1000\n",
    "\n",
    "# Small delay between requests to avoid hitting Reddit rate limits\n",
    "SLEEP = 0.4\n",
    "\n",
    "# Output file for the baseline scrape\n",
    "OUTPUT_BASE = \"depression_baseline_1000.csv\"\n",
    "\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Access the depression subreddit\n",
    "sub = reddit.subreddit(\"depression\")\n",
    "\n",
    "def collect_baseline_depression():\n",
    "    collected = []      # stores all scraped posts\n",
    "    seen_ids = set()    # used to avoid duplicates\n",
    "\n",
    "    print(\"Scraping baseline posts from r/depression...\")\n",
    "\n",
    "    # Loop through newest posts\n",
    "    for submission in sub.new(limit=None):\n",
    "\n",
    "        sid = submission.id\n",
    "\n",
    "        # Skip if we already collected this post\n",
    "        if sid in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(sid)\n",
    "\n",
    "        # Extract title and body safely\n",
    "        title = submission.title or \"\"\n",
    "        body = submission.selftext or \"\"\n",
    "\n",
    "        # Combine into one text field for later NLP use\n",
    "        full_text = (title + \"\\n\\n\" + body).strip()\n",
    "\n",
    "        # Skip empty posts\n",
    "        if full_text == \"\":\n",
    "            continue\n",
    "\n",
    "        # Convert Unix timestamp to readable datetime\n",
    "        created_dt = datetime.fromtimestamp(\n",
    "            submission.created_utc, tz=timezone.utc\n",
    "        ).isoformat()\n",
    "\n",
    "        # Store relevant fields in a dictionary\n",
    "        collected.append({\n",
    "            \"id\": sid,\n",
    "            \"subreddit\": submission.subreddit.display_name,\n",
    "            \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "            \"created_utc\": submission.created_utc,\n",
    "            \"created_dt\": created_dt,\n",
    "            \"title\": title,\n",
    "            \"selftext\": body,\n",
    "            \"full_text\": full_text,\n",
    "            \"score\": submission.score,\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"url\": submission.url,\n",
    "            \"link_flair_text\": submission.link_flair_text,\n",
    "            \"source\": \"baseline\"\n",
    "        })\n",
    "\n",
    "        # Progress update every 100 posts\n",
    "        if len(collected) % 100 == 0:\n",
    "            print(f\"Collected {len(collected)} baseline posts...\")\n",
    "\n",
    "        # Stop once we reach the target number\n",
    "        if len(collected) >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        # Small pause between keyword searches\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    # Convert results to DataFrame and save to CSV\n",
    "    df = pd.DataFrame(collected)\n",
    "    df.to_csv(OUTPUT_BASE, index=False)\n",
    "\n",
    "    print(f\"Saved baseline posts: {OUTPUT_BASE}\")\n",
    "    return df, seen_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After i have reached the maximum allowed posts to scrape with reddits API in one go, i define i function go through a recursive key-word based search to collect more posts. To avoid collecting the same post multiple times, I keep track of already seen post IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file for the keyword-based scrape\n",
    "OUTPUT_KEYWORD = \"depression_expanded_keywords.csv\"\n",
    "\n",
    "def keyword_expand_depression(df_existing, seen_ids, keywords, max_per_keyword=1000):\n",
    "    # Start from the already collected baseline rows\n",
    "    rows = df_existing.to_dict(\"records\")\n",
    "\n",
    "    print(\"\\nStarting keyword expansion...\")\n",
    "\n",
    "    # Loop through each keyword\n",
    "    for kw in keywords:\n",
    "        print(f\"Searching keyword: {kw!r}\")\n",
    "\n",
    "        try:\n",
    "            # Query Reddit for posts matching the keyword\n",
    "            for submission in sub.search(\n",
    "                query=kw,\n",
    "                sort=\"new\",\n",
    "                time_filter=\"all\",\n",
    "                limit=max_per_keyword,\n",
    "            ):\n",
    "                sid = submission.id\n",
    "\n",
    "                # Skip posts already collected\n",
    "                if sid in seen_ids:\n",
    "                    continue\n",
    "                seen_ids.add(sid)\n",
    "\n",
    "                # extract title and body\n",
    "                title = submission.title or \"\"\n",
    "                body = submission.selftext or \"\"\n",
    "                full_text = (title + \"\\n\\n\" + body).strip()\n",
    "\n",
    "                # Skip empty posts\n",
    "                if full_text == \"\":\n",
    "                    continue\n",
    "\n",
    "                # Convert timestamp to readable datetime\n",
    "                created_dt = datetime.fromtimestamp(\n",
    "                    submission.created_utc, tz=timezone.utc\n",
    "                ).isoformat()\n",
    "\n",
    "                # Store the post with the matching keyword\n",
    "                rows.append({\n",
    "                    \"id\": sid,\n",
    "                    \"subreddit\": submission.subreddit.display_name,\n",
    "                    \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                    \"created_utc\": submission.created_utc,\n",
    "                    \"created_dt\": created_dt,\n",
    "                    \"title\": title,\n",
    "                    \"selftext\": body,\n",
    "                    \"full_text\": full_text,\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"url\": submission.url,\n",
    "                    \"link_flair_text\": submission.link_flair_text,\n",
    "                    \"matched_keyword\": kw,\n",
    "                    \"source\": f\"keyword:{kw}\"\n",
    "                })\n",
    "\n",
    "        # Handle potential API or connection errors per keyword\n",
    "        except Exception as e:\n",
    "            print(f\"Error for keyword {kw}: {e}\")\n",
    "\n",
    "        # Progress update after each keyword\n",
    "        print(f\"Total unique collected so far: {len(seen_ids)}\")\n",
    "\n",
    "        # Small pause between keyword searches\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    # Save expanded dataset\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUTPUT_KEYWORD, index=False)\n",
    "\n",
    "    print(f\"\\nSaved: {OUTPUT_KEYWORD}\")\n",
    "    return df, seen_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The keywords selected to be used for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 keywords\n",
    "DEPRESSION_KEYWORDS = [\n",
    "    \"sad\", \"sadness\",\n",
    "    \"depressed\", \"depression\",\n",
    "    \"hopeless\", \"helpless\",\n",
    "    \"tired\", \"exhausted\",\n",
    "    \"empty\", \"numb\",\n",
    "    \"anxiety\", \"anxious\",\n",
    "    \"lonely\", \"alone\",\n",
    "    \"worthless\",\n",
    "    \"guilt\", \"guilty\",\n",
    "    \"ashamed\", \"shame\",\n",
    "    \"overwhelmed\",\n",
    "    \"panic\", \"fear\",\n",
    "    \"struggling\",\n",
    "    \"can't cope\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping baseline posts from r/depression...\n",
      "Collected 100 baseline posts...\n",
      "Collected 200 baseline posts...\n",
      "Collected 300 baseline posts...\n",
      "Collected 400 baseline posts...\n",
      "Collected 500 baseline posts...\n",
      "Collected 600 baseline posts...\n",
      "Collected 700 baseline posts...\n",
      "Collected 800 baseline posts...\n",
      "Collected 900 baseline posts...\n",
      "Saved baseline posts: depression_baseline_1000.csv\n",
      "\n",
      "Starting keyword expansion...\n",
      "Searching keyword: 'sad'\n",
      "Total unique collected so far: 1114\n",
      "Searching keyword: 'sadness'\n",
      "Total unique collected so far: 1114\n",
      "Searching keyword: 'depressed'\n",
      "Total unique collected so far: 1114\n",
      "Searching keyword: 'depression'\n",
      "Total unique collected so far: 1114\n",
      "Searching keyword: 'hopeless'\n",
      "Total unique collected so far: 1313\n",
      "Searching keyword: 'helpless'\n",
      "Total unique collected so far: 1533\n",
      "Searching keyword: 'tired'\n",
      "Total unique collected so far: 1629\n",
      "Searching keyword: 'exhausted'\n",
      "Total unique collected so far: 1785\n",
      "Searching keyword: 'empty'\n",
      "Total unique collected so far: 1932\n",
      "Searching keyword: 'numb'\n",
      "Total unique collected so far: 2086\n",
      "Searching keyword: 'anxiety'\n",
      "Total unique collected so far: 2184\n",
      "Searching keyword: 'anxious'\n",
      "Total unique collected so far: 2341\n",
      "Searching keyword: 'lonely'\n",
      "Total unique collected so far: 2460\n",
      "Searching keyword: 'alone'\n",
      "Total unique collected so far: 2490\n",
      "Searching keyword: 'worthless'\n",
      "Total unique collected so far: 2655\n",
      "Searching keyword: 'guilt'\n",
      "Total unique collected so far: 2828\n",
      "Searching keyword: 'guilty'\n",
      "Total unique collected so far: 2971\n",
      "Searching keyword: 'ashamed'\n",
      "Total unique collected so far: 3159\n",
      "Searching keyword: 'shame'\n",
      "Total unique collected so far: 3293\n",
      "Searching keyword: 'overwhelmed'\n",
      "Total unique collected so far: 3396\n",
      "Searching keyword: 'panic'\n",
      "Total unique collected so far: 3542\n",
      "Searching keyword: 'fear'\n",
      "Total unique collected so far: 3639\n",
      "Searching keyword: 'struggling'\n",
      "Total unique collected so far: 3683\n",
      "Searching keyword: \"can't cope\"\n",
      "Total unique collected so far: 3872\n",
      "\n",
      "Saved: depression_expanded_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "# 1000 posts\n",
    "df_base, seen_ids = collect_baseline_depression()\n",
    "\n",
    "# key-word based scraping\n",
    "df_full, seen_ids = keyword_expand_depression(df_base, seen_ids, DEPRESSION_KEYWORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scrape posts from r/CasualConversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The previous setup for scraping r/depression is reused for r/CasualConversation, but with different keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping baseline posts from r/CasualConversation...\n",
      "Collected 100 baseline posts...\n",
      "Collected 200 baseline posts...\n",
      "Collected 300 baseline posts...\n",
      "Collected 400 baseline posts...\n",
      "Collected 500 baseline posts...\n",
      "Collected 600 baseline posts...\n",
      "Collected 700 baseline posts...\n",
      "Collected 800 baseline posts...\n",
      "Collected 900 baseline posts...\n",
      "Saved baseline to casualconversation_baseline_1000.csv\n",
      "\n",
      "Starting keyword expansion for r/CasualConversation...\n",
      "\n",
      "Searching keyword: 'morning'\n",
      "Total unique posts so far: 1169\n",
      "\n",
      "Searching keyword: 'evening'\n",
      "Total unique posts so far: 1187\n",
      "\n",
      "Searching keyword: 'today'\n",
      "Total unique posts so far: 1297\n",
      "\n",
      "Searching keyword: 'yesterday'\n",
      "Total unique posts so far: 1496\n",
      "\n",
      "Searching keyword: 'weekend'\n",
      "Total unique posts so far: 1681\n",
      "\n",
      "Searching keyword: 'weekday'\n",
      "Total unique posts so far: 1867\n",
      "\n",
      "Searching keyword: 'work'\n",
      "Total unique posts so far: 1920\n",
      "\n",
      "Searching keyword: 'school'\n",
      "Total unique posts so far: 2082\n",
      "\n",
      "Searching keyword: 'job'\n",
      "Total unique posts so far: 2219\n",
      "\n",
      "Searching keyword: 'college'\n",
      "Total unique posts so far: 2376\n",
      "\n",
      "Searching keyword: 'friend'\n",
      "Total unique posts so far: 2412\n",
      "\n",
      "Searching keyword: 'friends'\n",
      "Total unique posts so far: 2412\n",
      "\n",
      "Searching keyword: 'family'\n",
      "Total unique posts so far: 2527\n",
      "\n",
      "Searching keyword: 'coworker'\n",
      "Total unique posts so far: 2715\n",
      "\n",
      "Searching keyword: 'hobby'\n",
      "Total unique posts so far: 2863\n",
      "\n",
      "Searching keyword: 'hobbies'\n",
      "Total unique posts so far: 2863\n",
      "\n",
      "Searching keyword: 'gaming'\n",
      "Total unique posts so far: 2983\n",
      "\n",
      "Searching keyword: 'game'\n",
      "Total unique posts so far: 2983\n",
      "\n",
      "Searching keyword: 'music'\n",
      "Total unique posts so far: 3095\n",
      "\n",
      "Searching keyword: 'movie'\n",
      "Total unique posts so far: 3194\n",
      "\n",
      "Searching keyword: 'food'\n",
      "Total unique posts so far: 3314\n",
      "\n",
      "Searching keyword: 'cooking'\n",
      "Total unique posts so far: 3448\n",
      "\n",
      "Searching keyword: 'travel'\n",
      "Total unique posts so far: 3602\n",
      "\n",
      "Searching keyword: 'weather'\n",
      "Total unique posts so far: 3794\n",
      "\n",
      "Searching keyword: 'discussion'\n",
      "Total unique posts so far: 3967\n",
      "\n",
      "Searching keyword: 'story'\n",
      "Total unique posts so far: 4051\n",
      "\n",
      "Saved full CC keyword+baseline dataset to casualconversation_expanded_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "TARGET_COUNT = 1000\n",
    "SLEEP = 0.4\n",
    "\n",
    "OUTPUT_BASE_CC = \"casualconversation_baseline_1000.csv\"\n",
    "OUTPUT_KEYWORD_CC = \"casualconversation_expanded_keywords.csv\"\n",
    "# -------------------------\n",
    "\n",
    "sub_cc = reddit.subreddit(\"CasualConversation\")\n",
    "\n",
    "# 24 keywords\n",
    "\n",
    "CASUAL_KEYWORDS = [\n",
    "    \"morning\", \"evening\",\n",
    "    \"today\", \"yesterday\",\n",
    "    \"weekend\", \"weekday\",\n",
    "\n",
    "    \"work\", \"school\",\n",
    "    \"job\", \"college\",\n",
    "\n",
    "    \"friend\", \"friends\",\n",
    "    \"family\", \"coworker\",\n",
    "\n",
    "    \"hobby\", \"hobbies\",\n",
    "    \"gaming\", \"game\",\n",
    "    \"music\", \"movie\",\n",
    "\n",
    "    \"food\", \"cooking\",\n",
    "    \"travel\", \"weather\",\n",
    "\n",
    "    \"discussion\", \"story\",\n",
    "]\n",
    "\n",
    "def collect_baseline_casual():\n",
    "    collected = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    print(\"Scraping baseline posts from r/CasualConversation...\")\n",
    "\n",
    "    for submission in sub_cc.new(limit=None):\n",
    "\n",
    "        sid = submission.id\n",
    "        if sid in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(sid)\n",
    "\n",
    "        title = submission.title or \"\"\n",
    "        body = submission.selftext or \"\"\n",
    "        full_text = (title + \"\\n\\n\" + body).strip()\n",
    "        if full_text == \"\":\n",
    "            continue\n",
    "\n",
    "        created_dt = datetime.fromtimestamp(\n",
    "            submission.created_utc, tz=timezone.utc\n",
    "        ).isoformat()\n",
    "\n",
    "        collected.append({\n",
    "            \"id\": sid,\n",
    "            \"subreddit\": submission.subreddit.display_name,\n",
    "            \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "            \"created_utc\": submission.created_utc,\n",
    "            \"created_dt\": created_dt,\n",
    "            \"title\": title,\n",
    "            \"selftext\": body,\n",
    "            \"full_text\": full_text,\n",
    "            \"score\": submission.score,\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"url\": submission.url,\n",
    "            \"link_flair_text\": submission.link_flair_text,\n",
    "            \"source\": \"baseline\",\n",
    "        })\n",
    "\n",
    "        if len(collected) % 100 == 0:\n",
    "            print(f\"Collected {len(collected)} baseline posts...\")\n",
    "\n",
    "        if len(collected) >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    df_base = pd.DataFrame(collected)\n",
    "    df_base.to_csv(OUTPUT_BASE_CC, index=False)\n",
    "    print(f\"Saved baseline to {OUTPUT_BASE_CC}\")\n",
    "\n",
    "    return df_base, seen_ids\n",
    "\n",
    "\n",
    "def keyword_expand_casual(df_existing, seen_ids, keywords, max_per_keyword=1000):\n",
    "    rows = df_existing.to_dict(\"records\")\n",
    "\n",
    "    print(\"\\nStarting keyword expansion for r/CasualConversation...\")\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(f\"\\nSearching keyword: {kw!r}\")\n",
    "\n",
    "        try:\n",
    "            for submission in sub_cc.search(\n",
    "                query=kw,\n",
    "                sort=\"new\",\n",
    "                time_filter=\"all\",\n",
    "                limit=max_per_keyword,\n",
    "            ):\n",
    "                sid = submission.id\n",
    "\n",
    "                if sid in seen_ids:\n",
    "                    continue\n",
    "                seen_ids.add(sid)\n",
    "\n",
    "                title = submission.title or \"\"\n",
    "                body = submission.selftext or \"\"\n",
    "                full_text = (title + \"\\n\\n\" + body).strip()\n",
    "                if full_text == \"\":\n",
    "                    continue\n",
    "\n",
    "                created_dt = datetime.fromtimestamp(\n",
    "                    submission.created_utc, tz=timezone.utc\n",
    "                ).isoformat()\n",
    "\n",
    "                rows.append({\n",
    "                    \"id\": sid,\n",
    "                    \"subreddit\": submission.subreddit.display_name,\n",
    "                    \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                    \"created_utc\": submission.created_utc,\n",
    "                    \"created_dt\": created_dt,\n",
    "                    \"title\": title,\n",
    "                    \"selftext\": body,\n",
    "                    \"full_text\": full_text,\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"url\": submission.url,\n",
    "                    \"link_flair_text\": submission.link_flair_text,\n",
    "                    \"matched_keyword\": kw,\n",
    "                    \"source\": f\"keyword:{kw}\",\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for keyword {kw}: {e}\")\n",
    "\n",
    "        print(f\"Total unique posts so far: {len(seen_ids)}\")\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    df_full = pd.DataFrame(rows)\n",
    "    df_full.to_csv(OUTPUT_KEYWORD_CC, index=False)\n",
    "    print(f\"\\nSaved full CC keyword+baseline dataset to {OUTPUT_KEYWORD_CC}\")\n",
    "\n",
    "    return df_full, seen_ids\n",
    "\n",
    "\n",
    "# scraping\n",
    "df_cc_base, cc_seen_ids = collect_baseline_casual()\n",
    "\n",
    "df_cc_full, cc_seen_ids = keyword_expand_casual(\n",
    "    df_cc_base,\n",
    "    cc_seen_ids,\n",
    "    CASUAL_KEYWORDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge datasets, label them and remove possible duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depression dataset: (3872, 15)\n"
     ]
    }
   ],
   "source": [
    "# load and merge datasets from r/depression\n",
    "df_dep_base = pd.read_csv(\"depression_baseline_1000.csv\")\n",
    "df_dep_kw   = pd.read_csv(\"depression_expanded_keywords.csv\")\n",
    "\n",
    "# merge + remove ID duplicates within depression\n",
    "df_dep = pd.concat([df_dep_base, df_dep_kw], ignore_index=True)\n",
    "df_dep = df_dep.drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "\n",
    "# label depression\n",
    "df_dep[\"label\"] = 1\n",
    "\n",
    "print(\"Depression dataset:\", df_dep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CasualConversation dataset: (4051, 15)\n"
     ]
    }
   ],
   "source": [
    "# do the same for casual conversation df\n",
    "df_cc_base = pd.read_csv(\"casualconversation_baseline_1000.csv\")\n",
    "df_cc_kw   = pd.read_csv(\"casualconversation_expanded_keywords.csv\")\n",
    "\n",
    "\n",
    "df_cc = pd.concat([df_cc_base, df_cc_kw], ignore_index=True)\n",
    "df_cc = df_cc.drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "\n",
    "df_cc[\"label\"] = 0\n",
    "\n",
    "print(\"CasualConversation dataset:\", df_cc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined rows (before global ID dedupe): 7923\n",
      "Removed due to duplicate ID: 0\n",
      "Rows after global ID dedupe: 7923\n",
      "Removed due to bad authors: 3\n",
      "Rows after author filter: 7920\n",
      "Removed placeholder texts: 0\n",
      "Rows after placeholder filter: 7920\n",
      "Removed due to duplicate full_text: 12\n",
      "Final cleaned rows: 7908\n",
      "Saved final cleaned file: clean_reddit.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine both labeled datasets\n",
    "df_final = pd.concat([df_dep, df_cc], ignore_index=True)\n",
    "print(\"Combined rows (before global ID dedupe):\", len(df_final))\n",
    "\n",
    "# 3.1 Global ID dedupe (just in case the same ID slipped into both sets)\n",
    "mask_dup_id = df_final.duplicated(subset=\"id\", keep=\"first\")\n",
    "dup_by_id = df_final[mask_dup_id].copy()\n",
    "df_final = df_final[~mask_dup_id].reset_index(drop=True)\n",
    "\n",
    "print(\"Removed due to duplicate ID:\", len(dup_by_id))\n",
    "print(\"Rows after global ID dedupe:\", len(df_final))\n",
    "\n",
    "# 3.2 Remove moderator/bot posts \n",
    "bad_authors = [\"AutoModerator\", \"moderator\", \"ModTeam\", \"CasualMods\"]\n",
    "\n",
    "mask_bad_authors = df_final[\"author\"].isin(bad_authors)\n",
    "removed_authors = df_final[mask_bad_authors].copy()\n",
    "df_final = df_final[~mask_bad_authors].reset_index(drop=True)\n",
    "\n",
    "print(\"Removed due to bad authors:\", len(removed_authors))\n",
    "print(\"Rows after author filter:\", len(df_final))\n",
    "\n",
    "# 3.3 Remove placeholder texts in full_text: [deleted], [removed]\n",
    "placeholder_texts = [\"[deleted]\", \"[removed]\"]\n",
    "\n",
    "mask_placeholders = (\n",
    "    df_final[\"full_text\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .isin(placeholder_texts)\n",
    ")\n",
    "removed_placeholders = df_final[mask_placeholders].copy()\n",
    "df_final = df_final[~mask_placeholders].reset_index(drop=True)\n",
    "\n",
    "print(\"Removed placeholder texts:\", len(removed_placeholders))\n",
    "print(\"Rows after placeholder filter:\", len(df_final))\n",
    "\n",
    "# 3.4 Remove textual duplicates (same full_text appearing multiple times)\n",
    "mask_dup_text = df_final.duplicated(subset=\"full_text\", keep=\"first\")\n",
    "dup_by_text = df_final[mask_dup_text].copy()  # these are the extra copies\n",
    "df_final_clean = df_final[~mask_dup_text].reset_index(drop=True)\n",
    "\n",
    "print(\"Removed due to duplicate full_text:\", len(dup_by_text))\n",
    "print(\"Final cleaned rows:\", len(df_final_clean))\n",
    "\n",
    "# Save final cleaned dataset\n",
    "df_final_clean.to_csv(\"clean_reddit.csv\", index=False)\n",
    "print(\"Saved final cleaned file: clean_reddit.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check exactly what was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Moderator comments removed ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_dt</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>source</th>\n",
       "      <th>matched_keyword</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>1p6eml9</td>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>CasualMods</td>\n",
       "      <td>1.764083e+09</td>\n",
       "      <td>2025-11-25T15:00:35+00:00</td>\n",
       "      <td>r/CasualConversation resources, rules, etiquet...</td>\n",
       "      <td>We have a lot to offer in terms of reading ma...</td>\n",
       "      <td>r/CasualConversation resources, rules, etiquet...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/CasualConversation/co...</td>\n",
       "      <td>:cc: Meta</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699</th>\n",
       "      <td>1p0e5ge</td>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>CasualMods</td>\n",
       "      <td>1.763478e+09</td>\n",
       "      <td>2025-11-18T15:00:29+00:00</td>\n",
       "      <td>Join r/CasualConversation on Twitter and IRC!</td>\n",
       "      <td>Our subreddit has many ways to keep in touch w...</td>\n",
       "      <td>Join r/CasualConversation on Twitter and IRC!\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/CasualConversation/co...</td>\n",
       "      <td>:cc: Meta</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5780</th>\n",
       "      <td>1oubmj0</td>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>CasualMods</td>\n",
       "      <td>1.762873e+09</td>\n",
       "      <td>2025-11-11T15:00:32+00:00</td>\n",
       "      <td>r/CasualConversation resources, rules, etiquet...</td>\n",
       "      <td>We have a lot to offer in terms of reading ma...</td>\n",
       "      <td>r/CasualConversation resources, rules, etiquet...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/CasualConversation/co...</td>\n",
       "      <td>:cc: Meta</td>\n",
       "      <td>keyword:work</td>\n",
       "      <td>work</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id           subreddit      author   created_utc  \\\n",
       "4204  1p6eml9  CasualConversation  CasualMods  1.764083e+09   \n",
       "4699  1p0e5ge  CasualConversation  CasualMods  1.763478e+09   \n",
       "5780  1oubmj0  CasualConversation  CasualMods  1.762873e+09   \n",
       "\n",
       "                     created_dt  \\\n",
       "4204  2025-11-25T15:00:35+00:00   \n",
       "4699  2025-11-18T15:00:29+00:00   \n",
       "5780  2025-11-11T15:00:32+00:00   \n",
       "\n",
       "                                                  title  \\\n",
       "4204  r/CasualConversation resources, rules, etiquet...   \n",
       "4699      Join r/CasualConversation on Twitter and IRC!   \n",
       "5780  r/CasualConversation resources, rules, etiquet...   \n",
       "\n",
       "                                               selftext  \\\n",
       "4204   We have a lot to offer in terms of reading ma...   \n",
       "4699  Our subreddit has many ways to keep in touch w...   \n",
       "5780   We have a lot to offer in terms of reading ma...   \n",
       "\n",
       "                                              full_text  score  num_comments  \\\n",
       "4204  r/CasualConversation resources, rules, etiquet...      2             1   \n",
       "4699  Join r/CasualConversation on Twitter and IRC!\\...      1             0   \n",
       "5780  r/CasualConversation resources, rules, etiquet...      1             0   \n",
       "\n",
       "                                                    url link_flair_text  \\\n",
       "4204  https://www.reddit.com/r/CasualConversation/co...       :cc: Meta   \n",
       "4699  https://www.reddit.com/r/CasualConversation/co...       :cc: Meta   \n",
       "5780  https://www.reddit.com/r/CasualConversation/co...       :cc: Meta   \n",
       "\n",
       "            source matched_keyword  label  \n",
       "4204      baseline             NaN      0  \n",
       "4699      baseline             NaN      0  \n",
       "5780  keyword:work            work      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== removed duplicate texts ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_dt</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>source</th>\n",
       "      <th>matched_keyword</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>1p7fkd8</td>\n",
       "      <td>depression</td>\n",
       "      <td>Dry_Lock1015</td>\n",
       "      <td>1.764182e+09</td>\n",
       "      <td>2025-11-26T18:33:48+00:00</td>\n",
       "      <td>i hate me</td>\n",
       "      <td>Right now I’m at the movies with friends, yes ...</td>\n",
       "      <td>i hate me\\n\\nRight now I’m at the movies with ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>1p48yb6</td>\n",
       "      <td>depression</td>\n",
       "      <td>United-Ingenuity8372</td>\n",
       "      <td>1.763858e+09</td>\n",
       "      <td>2025-11-23T00:37:46+00:00</td>\n",
       "      <td>My 23m gf 21f antidepressants or depression ca...</td>\n",
       "      <td>I am dating the most amazing girl I’ve ever me...</td>\n",
       "      <td>My 23m gf 21f antidepressants or depression ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keyword:exhausted</td>\n",
       "      <td>exhausted</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1otr8au</td>\n",
       "      <td>depression</td>\n",
       "      <td>United-Ingenuity8372</td>\n",
       "      <td>1.762812e+09</td>\n",
       "      <td>2025-11-10T21:53:15+00:00</td>\n",
       "      <td>My 23m gf 21f antidepressants or depression ca...</td>\n",
       "      <td>I am dating the most amazing girl I’ve ever me...</td>\n",
       "      <td>My 23m gf 21f antidepressants or depression ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keyword:exhausted</td>\n",
       "      <td>exhausted</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>1p0u78y</td>\n",
       "      <td>depression</td>\n",
       "      <td>SageTheLynx</td>\n",
       "      <td>1.763516e+09</td>\n",
       "      <td>2025-11-19T01:25:13+00:00</td>\n",
       "      <td>My friend tried to kill herself again and I ju...</td>\n",
       "      <td>I know she feels worse. I know how awful she f...</td>\n",
       "      <td>My friend tried to kill herself again and I ju...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keyword:lonely</td>\n",
       "      <td>lonely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>1o5crk8</td>\n",
       "      <td>depression</td>\n",
       "      <td>DegreeLogical1248</td>\n",
       "      <td>1.760339e+09</td>\n",
       "      <td>2025-10-13T07:00:55+00:00</td>\n",
       "      <td>I hurt my dad, betrayed his trust, and it’s te...</td>\n",
       "      <td>I don’t even know where to start. I feel compl...</td>\n",
       "      <td>I hurt my dad, betrayed his trust, and it’s te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keyword:guilt</td>\n",
       "      <td>guilt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>1nqb50i</td>\n",
       "      <td>depression</td>\n",
       "      <td>_iwtd</td>\n",
       "      <td>1.758817e+09</td>\n",
       "      <td>2025-09-25T16:18:15+00:00</td>\n",
       "      <td>I understand but I don’t understand</td>\n",
       "      <td>When someone is suicidal everyone tells him th...</td>\n",
       "      <td>I understand but I don’t understand\\n\\nWhen so...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keyword:guilty</td>\n",
       "      <td>guilty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3699</th>\n",
       "      <td>1ojgd0q</td>\n",
       "      <td>depression</td>\n",
       "      <td>FriendlyHighway1363</td>\n",
       "      <td>1.761771e+09</td>\n",
       "      <td>2025-10-29T20:55:51+00:00</td>\n",
       "      <td>Living for other people. But why?</td>\n",
       "      <td>Skip To QUESTION If you want. It's the 2nd las...</td>\n",
       "      <td>Living for other people. But why?\\n\\nSkip To Q...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keyword:can't cope</td>\n",
       "      <td>can't cope</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>1o449jf</td>\n",
       "      <td>depression</td>\n",
       "      <td>No-Advice2384</td>\n",
       "      <td>1.760210e+09</td>\n",
       "      <td>2025-10-11T19:20:13+00:00</td>\n",
       "      <td>I experience a lot of Mood swings recently. Do...</td>\n",
       "      <td>Some context:\\nIm 17.5 years old female.\\nI ha...</td>\n",
       "      <td>I experience a lot of Mood swings recently. Do...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keyword:can't cope</td>\n",
       "      <td>can't cope</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6594</th>\n",
       "      <td>1onu8yl</td>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>Weirdo_the_dino</td>\n",
       "      <td>1.762219e+09</td>\n",
       "      <td>2025-11-04T01:20:13+00:00</td>\n",
       "      <td>How to help perfectionism?</td>\n",
       "      <td>I've become such a perfectionist that i find n...</td>\n",
       "      <td>How to help perfectionism?\\n\\nI've become such...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/CasualConversation/co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keyword:hobby</td>\n",
       "      <td>hobby</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>1nkphvw</td>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>JealousMap3475</td>\n",
       "      <td>1.758243e+09</td>\n",
       "      <td>2025-09-19T00:47:32+00:00</td>\n",
       "      <td>17m, bored, numbers game anyone?</td>\n",
       "      <td>17m, bored, numbers game anyone?\\n\\nAround my ...</td>\n",
       "      <td>17m, bored, numbers game anyone?\\n\\n17m, bored...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/CasualConversation/co...</td>\n",
       "      <td>:chat: Just Chatting</td>\n",
       "      <td>keyword:gaming</td>\n",
       "      <td>gaming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>1nj24b6</td>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>JealousMap3475</td>\n",
       "      <td>1.758079e+09</td>\n",
       "      <td>2025-09-17T03:13:57+00:00</td>\n",
       "      <td>17m, bored, numbers game anyone?</td>\n",
       "      <td>17m, bored, numbers game anyone?\\n\\nAround my ...</td>\n",
       "      <td>17m, bored, numbers game anyone?\\n\\n17m, bored...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.reddit.com/r/CasualConversation/co...</td>\n",
       "      <td>:chat: Just Chatting</td>\n",
       "      <td>keyword:gaming</td>\n",
       "      <td>gaming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>1oeacu7</td>\n",
       "      <td>CasualConversation</td>\n",
       "      <td>SilverWeb3944</td>\n",
       "      <td>1.761242e+09</td>\n",
       "      <td>2025-10-23T18:00:28+00:00</td>\n",
       "      <td>What are the best movies to watch during the f...</td>\n",
       "      <td>We all love fall!! What are the best movies to...</td>\n",
       "      <td>What are the best movies to watch during the f...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reddit.com/r/CasualConversation/co...</td>\n",
       "      <td>:film: Movies &amp; Shows</td>\n",
       "      <td>keyword:movie</td>\n",
       "      <td>movie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id           subreddit                author   created_utc  \\\n",
       "534   1p7fkd8          depression          Dry_Lock1015  1.764182e+09   \n",
       "1636  1p48yb6          depression  United-Ingenuity8372  1.763858e+09   \n",
       "1729  1otr8au          depression  United-Ingenuity8372  1.762812e+09   \n",
       "2366  1p0u78y          depression           SageTheLynx  1.763516e+09   \n",
       "2746  1o5crk8          depression     DegreeLogical1248  1.760339e+09   \n",
       "2964  1nqb50i          depression                 _iwtd  1.758817e+09   \n",
       "3699  1ojgd0q          depression   FriendlyHighway1363  1.761771e+09   \n",
       "3714  1o449jf          depression         No-Advice2384  1.760210e+09   \n",
       "6594  1onu8yl  CasualConversation       Weirdo_the_dino  1.762219e+09   \n",
       "6832  1nkphvw  CasualConversation        JealousMap3475  1.758243e+09   \n",
       "6836  1nj24b6  CasualConversation        JealousMap3475  1.758079e+09   \n",
       "7011  1oeacu7  CasualConversation         SilverWeb3944  1.761242e+09   \n",
       "\n",
       "                     created_dt  \\\n",
       "534   2025-11-26T18:33:48+00:00   \n",
       "1636  2025-11-23T00:37:46+00:00   \n",
       "1729  2025-11-10T21:53:15+00:00   \n",
       "2366  2025-11-19T01:25:13+00:00   \n",
       "2746  2025-10-13T07:00:55+00:00   \n",
       "2964  2025-09-25T16:18:15+00:00   \n",
       "3699  2025-10-29T20:55:51+00:00   \n",
       "3714  2025-10-11T19:20:13+00:00   \n",
       "6594  2025-11-04T01:20:13+00:00   \n",
       "6832  2025-09-19T00:47:32+00:00   \n",
       "6836  2025-09-17T03:13:57+00:00   \n",
       "7011  2025-10-23T18:00:28+00:00   \n",
       "\n",
       "                                                  title  \\\n",
       "534                                           i hate me   \n",
       "1636  My 23m gf 21f antidepressants or depression ca...   \n",
       "1729  My 23m gf 21f antidepressants or depression ca...   \n",
       "2366  My friend tried to kill herself again and I ju...   \n",
       "2746  I hurt my dad, betrayed his trust, and it’s te...   \n",
       "2964                I understand but I don’t understand   \n",
       "3699                  Living for other people. But why?   \n",
       "3714  I experience a lot of Mood swings recently. Do...   \n",
       "6594                         How to help perfectionism?   \n",
       "6832                   17m, bored, numbers game anyone?   \n",
       "6836                   17m, bored, numbers game anyone?   \n",
       "7011  What are the best movies to watch during the f...   \n",
       "\n",
       "                                               selftext  \\\n",
       "534   Right now I’m at the movies with friends, yes ...   \n",
       "1636  I am dating the most amazing girl I’ve ever me...   \n",
       "1729  I am dating the most amazing girl I’ve ever me...   \n",
       "2366  I know she feels worse. I know how awful she f...   \n",
       "2746  I don’t even know where to start. I feel compl...   \n",
       "2964  When someone is suicidal everyone tells him th...   \n",
       "3699  Skip To QUESTION If you want. It's the 2nd las...   \n",
       "3714  Some context:\\nIm 17.5 years old female.\\nI ha...   \n",
       "6594  I've become such a perfectionist that i find n...   \n",
       "6832  17m, bored, numbers game anyone?\\n\\nAround my ...   \n",
       "6836  17m, bored, numbers game anyone?\\n\\nAround my ...   \n",
       "7011  We all love fall!! What are the best movies to...   \n",
       "\n",
       "                                              full_text  score  num_comments  \\\n",
       "534   i hate me\\n\\nRight now I’m at the movies with ...      2             1   \n",
       "1636  My 23m gf 21f antidepressants or depression ca...      1             1   \n",
       "1729  My 23m gf 21f antidepressants or depression ca...      0             3   \n",
       "2366  My friend tried to kill herself again and I ju...      2             0   \n",
       "2746  I hurt my dad, betrayed his trust, and it’s te...      1             0   \n",
       "2964  I understand but I don’t understand\\n\\nWhen so...      2             1   \n",
       "3699  Living for other people. But why?\\n\\nSkip To Q...      1             0   \n",
       "3714  I experience a lot of Mood swings recently. Do...      1             0   \n",
       "6594  How to help perfectionism?\\n\\nI've become such...      0             1   \n",
       "6832  17m, bored, numbers game anyone?\\n\\n17m, bored...      0             4   \n",
       "6836  17m, bored, numbers game anyone?\\n\\n17m, bored...      3             7   \n",
       "7011  What are the best movies to watch during the f...      0             3   \n",
       "\n",
       "                                                    url  \\\n",
       "534   https://www.reddit.com/r/depression/comments/1...   \n",
       "1636  https://www.reddit.com/r/depression/comments/1...   \n",
       "1729  https://www.reddit.com/r/depression/comments/1...   \n",
       "2366  https://www.reddit.com/r/depression/comments/1...   \n",
       "2746  https://www.reddit.com/r/depression/comments/1...   \n",
       "2964  https://www.reddit.com/r/depression/comments/1...   \n",
       "3699  https://www.reddit.com/r/depression/comments/1...   \n",
       "3714  https://www.reddit.com/r/depression/comments/1...   \n",
       "6594  https://www.reddit.com/r/CasualConversation/co...   \n",
       "6832  https://www.reddit.com/r/CasualConversation/co...   \n",
       "6836  https://www.reddit.com/r/CasualConversation/co...   \n",
       "7011  https://www.reddit.com/r/CasualConversation/co...   \n",
       "\n",
       "            link_flair_text              source matched_keyword  label  \n",
       "534                     NaN            baseline             NaN      1  \n",
       "1636                    NaN   keyword:exhausted       exhausted      1  \n",
       "1729                    NaN   keyword:exhausted       exhausted      1  \n",
       "2366                    NaN      keyword:lonely          lonely      1  \n",
       "2746                    NaN       keyword:guilt           guilt      1  \n",
       "2964                    NaN      keyword:guilty          guilty      1  \n",
       "3699                    NaN  keyword:can't cope      can't cope      1  \n",
       "3714                    NaN  keyword:can't cope      can't cope      1  \n",
       "6594                    NaN       keyword:hobby           hobby      0  \n",
       "6832   :chat: Just Chatting      keyword:gaming          gaming      0  \n",
       "6836   :chat: Just Chatting      keyword:gaming          gaming      0  \n",
       "7011  :film: Movies & Shows       keyword:movie           movie      0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== Moderator comments removed ===\")\n",
    "display(removed_authors)\n",
    "\n",
    "print(\"\\n=== removed duplicate texts ===\") # these are the extra copies of the duplicates that have been removed\n",
    "dup_by_text.head(12)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

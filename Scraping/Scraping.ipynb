{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for scraping reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from datetime import timezone\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is my API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# The weird letters are the cresidentials from my API\n",
    "\n",
    "reddit = praw.Reddit(client_id ='hQafbgnPnl6Jl_wabPofuA',\n",
    "                     client_secret ='OZ0EsYse7J-JiYE7kPk3mddQgMZ2nA',\n",
    "                     user_agent ='WebScraper/Own-Biscotti6249')\n",
    "                     \n",
    "# to verify whether the instance is authorized instance or not\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r/depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 POSTS\n",
    "\n",
    "TARGET_COUNT = 1000\n",
    "SLEEP = 0.4\n",
    "OUTPUT_BASE = \"depression_baseline_1000.csv\"\n",
    "# -------------------------\n",
    "\n",
    "sub = reddit.subreddit(\"depression\")\n",
    "\n",
    "\n",
    "def collect_baseline_depression():\n",
    "    collected = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    print(\"Scraping baseline posts from r/depression...\")\n",
    "\n",
    "    for submission in sub.new(limit=None):\n",
    "\n",
    "        sid = submission.id\n",
    "        if sid in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(sid)\n",
    "\n",
    "        title = submission.title or \"\"\n",
    "        body = submission.selftext or \"\"\n",
    "        full_text = (title + \"\\n\\n\" + body).strip()\n",
    "\n",
    "        if full_text == \"\":\n",
    "            continue\n",
    "\n",
    "        created_dt = datetime.fromtimestamp(\n",
    "            submission.created_utc, tz=timezone.utc\n",
    "        ).isoformat()\n",
    "\n",
    "        collected.append({\n",
    "            \"id\": sid,\n",
    "            \"subreddit\": submission.subreddit.display_name,\n",
    "            \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "            \"created_utc\": submission.created_utc,\n",
    "            \"created_dt\": created_dt,\n",
    "            \"title\": title,\n",
    "            \"selftext\": body,\n",
    "            \"full_text\": full_text,\n",
    "            \"score\": submission.score,\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"url\": submission.url,\n",
    "            \"link_flair_text\": submission.link_flair_text,\n",
    "            \"source\": \"baseline\"\n",
    "        })\n",
    "\n",
    "        if len(collected) % 100 == 0:\n",
    "            print(f\"Collected {len(collected)} baseline posts...\")\n",
    "\n",
    "        if len(collected) >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    df = pd.DataFrame(collected)\n",
    "    df.to_csv(OUTPUT_BASE, index=False)\n",
    "\n",
    "    print(f\"Saved baseline posts: {OUTPUT_BASE}\")\n",
    "    return df, seen_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEYWORD\n",
    "OUTPUT_KEYWORD = \"depression_expanded_keywords.csv\"\n",
    "\n",
    "def keyword_expand_depression(df_existing, seen_ids, keywords, max_per_keyword=1000):\n",
    "    rows = df_existing.to_dict(\"records\")\n",
    "\n",
    "    print(\"\\nStarting keyword expansion...\")\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(f\"Searching keyword: {kw!r}\")\n",
    "        try:\n",
    "            for submission in sub.search(\n",
    "                query=kw,\n",
    "                sort=\"new\",\n",
    "                time_filter=\"all\",\n",
    "                limit=max_per_keyword,\n",
    "            ):\n",
    "                sid = submission.id\n",
    "                if sid in seen_ids:\n",
    "                    continue\n",
    "                seen_ids.add(sid)\n",
    "\n",
    "                title = submission.title or \"\"\n",
    "                body = submission.selftext or \"\"\n",
    "                full_text = (title + \"\\n\\n\" + body).strip()\n",
    "                if full_text == \"\":\n",
    "                    continue\n",
    "\n",
    "                created_dt = datetime.fromtimestamp(\n",
    "                    submission.created_utc, tz=timezone.utc\n",
    "                ).isoformat()\n",
    "\n",
    "                rows.append({\n",
    "                    \"id\": sid,\n",
    "                    \"subreddit\": submission.subreddit.display_name,\n",
    "                    \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                    \"created_utc\": submission.created_utc,\n",
    "                    \"created_dt\": created_dt,\n",
    "                    \"title\": title,\n",
    "                    \"selftext\": body,\n",
    "                    \"full_text\": full_text,\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"url\": submission.url,\n",
    "                    \"link_flair_text\": submission.link_flair_text,\n",
    "                    \"matched_keyword\": kw,\n",
    "                    \"source\": f\"keyword:{kw}\"\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error for keyword {kw}: {e}\")\n",
    "\n",
    "        print(f\"Total unique collected so far: {len(seen_ids)}\")\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUTPUT_KEYWORD, index=False)\n",
    "    print(f\"\\nSaved: {OUTPUT_KEYWORD}\")\n",
    "\n",
    "    return df, seen_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 keywords\n",
    "DEPRESSION_KEYWORDS = [\n",
    "    \"sad\", \"sadness\",\n",
    "    \"depressed\", \"depression\",\n",
    "    \"hopeless\", \"helpless\",\n",
    "    \"tired\", \"exhausted\",\n",
    "    \"empty\", \"numb\",\n",
    "    \"anxiety\", \"anxious\",\n",
    "    \"lonely\", \"alone\",\n",
    "    \"worthless\",\n",
    "    \"guilt\", \"guilty\",\n",
    "    \"ashamed\", \"shame\",\n",
    "    \"overwhelmed\",\n",
    "    \"panic\", \"fear\",\n",
    "    \"struggling\",\n",
    "    \"can't cope\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping baseline posts from r/depression...\n",
      "Collected 100 baseline posts...\n",
      "Collected 200 baseline posts...\n",
      "Collected 300 baseline posts...\n",
      "Collected 400 baseline posts...\n",
      "Collected 500 baseline posts...\n",
      "Collected 600 baseline posts...\n",
      "Collected 700 baseline posts...\n",
      "Collected 800 baseline posts...\n",
      "Collected 900 baseline posts...\n",
      "Saved baseline posts: depression_baseline_1000.csv\n",
      "\n",
      "Starting keyword expansion...\n",
      "Searching keyword: 'sad'\n",
      "Total unique collected so far: 1114\n",
      "Searching keyword: 'sadness'\n",
      "Total unique collected so far: 1114\n",
      "Searching keyword: 'depressed'\n",
      "Total unique collected so far: 1114\n",
      "Searching keyword: 'depression'\n",
      "Total unique collected so far: 1114\n",
      "Searching keyword: 'hopeless'\n",
      "Total unique collected so far: 1313\n",
      "Searching keyword: 'helpless'\n",
      "Total unique collected so far: 1533\n",
      "Searching keyword: 'tired'\n",
      "Total unique collected so far: 1629\n",
      "Searching keyword: 'exhausted'\n",
      "Total unique collected so far: 1785\n",
      "Searching keyword: 'empty'\n",
      "Total unique collected so far: 1932\n",
      "Searching keyword: 'numb'\n",
      "Total unique collected so far: 2086\n",
      "Searching keyword: 'anxiety'\n",
      "Total unique collected so far: 2184\n",
      "Searching keyword: 'anxious'\n",
      "Total unique collected so far: 2341\n",
      "Searching keyword: 'lonely'\n",
      "Total unique collected so far: 2460\n",
      "Searching keyword: 'alone'\n",
      "Total unique collected so far: 2490\n",
      "Searching keyword: 'worthless'\n",
      "Total unique collected so far: 2655\n",
      "Searching keyword: 'guilt'\n",
      "Total unique collected so far: 2828\n",
      "Searching keyword: 'guilty'\n",
      "Total unique collected so far: 2971\n",
      "Searching keyword: 'ashamed'\n",
      "Total unique collected so far: 3159\n",
      "Searching keyword: 'shame'\n",
      "Total unique collected so far: 3293\n",
      "Searching keyword: 'overwhelmed'\n",
      "Total unique collected so far: 3396\n",
      "Searching keyword: 'panic'\n",
      "Total unique collected so far: 3542\n",
      "Searching keyword: 'fear'\n",
      "Total unique collected so far: 3639\n",
      "Searching keyword: 'struggling'\n",
      "Total unique collected so far: 3683\n",
      "Searching keyword: \"can't cope\"\n",
      "Total unique collected so far: 3872\n",
      "\n",
      "Saved: depression_expanded_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - baseline\n",
    "df_base, seen_ids = collect_baseline_depression()\n",
    "\n",
    "# Step 2 - keyword expansion\n",
    "df_full, seen_ids = keyword_expand_depression(df_base, seen_ids, DEPRESSION_KEYWORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r/casualconversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping baseline posts from r/CasualConversation...\n",
      "Collected 100 baseline posts...\n",
      "Collected 200 baseline posts...\n",
      "Collected 300 baseline posts...\n",
      "Collected 400 baseline posts...\n",
      "Collected 500 baseline posts...\n",
      "Collected 600 baseline posts...\n",
      "Collected 700 baseline posts...\n",
      "Collected 800 baseline posts...\n",
      "Collected 900 baseline posts...\n",
      "Saved baseline to casualconversation_baseline_1000.csv\n",
      "\n",
      "Starting keyword expansion for r/CasualConversation...\n",
      "\n",
      "Searching keyword: 'morning'\n",
      "Total unique posts so far: 1169\n",
      "\n",
      "Searching keyword: 'evening'\n",
      "Total unique posts so far: 1187\n",
      "\n",
      "Searching keyword: 'today'\n",
      "Total unique posts so far: 1297\n",
      "\n",
      "Searching keyword: 'yesterday'\n",
      "Total unique posts so far: 1496\n",
      "\n",
      "Searching keyword: 'weekend'\n",
      "Total unique posts so far: 1681\n",
      "\n",
      "Searching keyword: 'weekday'\n",
      "Total unique posts so far: 1867\n",
      "\n",
      "Searching keyword: 'work'\n",
      "Total unique posts so far: 1920\n",
      "\n",
      "Searching keyword: 'school'\n",
      "Total unique posts so far: 2082\n",
      "\n",
      "Searching keyword: 'job'\n",
      "Total unique posts so far: 2219\n",
      "\n",
      "Searching keyword: 'college'\n",
      "Total unique posts so far: 2376\n",
      "\n",
      "Searching keyword: 'friend'\n",
      "Total unique posts so far: 2412\n",
      "\n",
      "Searching keyword: 'friends'\n",
      "Total unique posts so far: 2412\n",
      "\n",
      "Searching keyword: 'family'\n",
      "Total unique posts so far: 2527\n",
      "\n",
      "Searching keyword: 'coworker'\n",
      "Total unique posts so far: 2715\n",
      "\n",
      "Searching keyword: 'hobby'\n",
      "Total unique posts so far: 2863\n",
      "\n",
      "Searching keyword: 'hobbies'\n",
      "Total unique posts so far: 2863\n",
      "\n",
      "Searching keyword: 'gaming'\n",
      "Total unique posts so far: 2983\n",
      "\n",
      "Searching keyword: 'game'\n",
      "Total unique posts so far: 2983\n",
      "\n",
      "Searching keyword: 'music'\n",
      "Total unique posts so far: 3095\n",
      "\n",
      "Searching keyword: 'movie'\n",
      "Total unique posts so far: 3194\n",
      "\n",
      "Searching keyword: 'food'\n",
      "Total unique posts so far: 3314\n",
      "\n",
      "Searching keyword: 'cooking'\n",
      "Total unique posts so far: 3448\n",
      "\n",
      "Searching keyword: 'travel'\n",
      "Total unique posts so far: 3602\n",
      "\n",
      "Searching keyword: 'weather'\n",
      "Total unique posts so far: 3794\n",
      "\n",
      "Searching keyword: 'discussion'\n",
      "Total unique posts so far: 3967\n",
      "\n",
      "Searching keyword: 'story'\n",
      "Total unique posts so far: 4051\n",
      "\n",
      "Saved full CC keyword+baseline dataset to casualconversation_expanded_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import praw\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "TARGET_COUNT = 1000\n",
    "SLEEP = 0.4\n",
    "\n",
    "OUTPUT_BASE_CC = \"casualconversation_baseline_1000.csv\"\n",
    "OUTPUT_KEYWORD_CC = \"casualconversation_expanded_keywords.csv\"\n",
    "# -------------------------\n",
    "\n",
    "sub_cc = reddit.subreddit(\"CasualConversation\")\n",
    "\n",
    "# -------------------------\n",
    "# 24 keywords\n",
    "# -------------------------\n",
    "CASUAL_KEYWORDS = [\n",
    "    \"morning\", \"evening\",\n",
    "    \"today\", \"yesterday\",\n",
    "    \"weekend\", \"weekday\",\n",
    "\n",
    "    \"work\", \"school\",\n",
    "    \"job\", \"college\",\n",
    "\n",
    "    \"friend\", \"friends\",\n",
    "    \"family\", \"coworker\",\n",
    "\n",
    "    \"hobby\", \"hobbies\",\n",
    "    \"gaming\", \"game\",\n",
    "    \"music\", \"movie\",\n",
    "\n",
    "    \"food\", \"cooking\",\n",
    "    \"travel\", \"weather\",\n",
    "\n",
    "    \"discussion\", \"story\",\n",
    "]\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "def collect_baseline_casual():\n",
    "    collected = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    print(\"Scraping baseline posts from r/CasualConversation...\")\n",
    "\n",
    "    for submission in sub_cc.new(limit=None):\n",
    "\n",
    "        sid = submission.id\n",
    "        if sid in seen_ids:\n",
    "            continue\n",
    "        seen_ids.add(sid)\n",
    "\n",
    "        title = submission.title or \"\"\n",
    "        body = submission.selftext or \"\"\n",
    "        full_text = (title + \"\\n\\n\" + body).strip()\n",
    "        if full_text == \"\":\n",
    "            continue\n",
    "\n",
    "        created_dt = datetime.fromtimestamp(\n",
    "            submission.created_utc, tz=timezone.utc\n",
    "        ).isoformat()\n",
    "\n",
    "        collected.append({\n",
    "            \"id\": sid,\n",
    "            \"subreddit\": submission.subreddit.display_name,\n",
    "            \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "            \"created_utc\": submission.created_utc,\n",
    "            \"created_dt\": created_dt,\n",
    "            \"title\": title,\n",
    "            \"selftext\": body,\n",
    "            \"full_text\": full_text,\n",
    "            \"score\": submission.score,\n",
    "            \"num_comments\": submission.num_comments,\n",
    "            \"url\": submission.url,\n",
    "            \"link_flair_text\": submission.link_flair_text,\n",
    "            \"source\": \"baseline\",\n",
    "        })\n",
    "\n",
    "        if len(collected) % 100 == 0:\n",
    "            print(f\"Collected {len(collected)} baseline posts...\")\n",
    "\n",
    "        if len(collected) >= TARGET_COUNT:\n",
    "            break\n",
    "\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    df_base = pd.DataFrame(collected)\n",
    "    df_base.to_csv(OUTPUT_BASE_CC, index=False)\n",
    "    print(f\"Saved baseline to {OUTPUT_BASE_CC}\")\n",
    "\n",
    "    return df_base, seen_ids\n",
    "\n",
    "\n",
    "def keyword_expand_casual(df_existing, seen_ids, keywords, max_per_keyword=1000):\n",
    "    rows = df_existing.to_dict(\"records\")\n",
    "\n",
    "    print(\"\\nStarting keyword expansion for r/CasualConversation...\")\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(f\"\\nSearching keyword: {kw!r}\")\n",
    "\n",
    "        try:\n",
    "            for submission in sub_cc.search(\n",
    "                query=kw,\n",
    "                sort=\"new\",\n",
    "                time_filter=\"all\",\n",
    "                limit=max_per_keyword,\n",
    "            ):\n",
    "                sid = submission.id\n",
    "\n",
    "                if sid in seen_ids:\n",
    "                    continue\n",
    "                seen_ids.add(sid)\n",
    "\n",
    "                title = submission.title or \"\"\n",
    "                body = submission.selftext or \"\"\n",
    "                full_text = (title + \"\\n\\n\" + body).strip()\n",
    "                if full_text == \"\":\n",
    "                    continue\n",
    "\n",
    "                created_dt = datetime.fromtimestamp(\n",
    "                    submission.created_utc, tz=timezone.utc\n",
    "                ).isoformat()\n",
    "\n",
    "                rows.append({\n",
    "                    \"id\": sid,\n",
    "                    \"subreddit\": submission.subreddit.display_name,\n",
    "                    \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                    \"created_utc\": submission.created_utc,\n",
    "                    \"created_dt\": created_dt,\n",
    "                    \"title\": title,\n",
    "                    \"selftext\": body,\n",
    "                    \"full_text\": full_text,\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"url\": submission.url,\n",
    "                    \"link_flair_text\": submission.link_flair_text,\n",
    "                    \"matched_keyword\": kw,\n",
    "                    \"source\": f\"keyword:{kw}\",\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for keyword {kw}: {e}\")\n",
    "\n",
    "        print(f\"Total unique posts so far: {len(seen_ids)}\")\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    df_full = pd.DataFrame(rows)\n",
    "    df_full.to_csv(OUTPUT_KEYWORD_CC, index=False)\n",
    "    print(f\"\\nSaved full CC keyword+baseline dataset to {OUTPUT_KEYWORD_CC}\")\n",
    "\n",
    "    return df_full, seen_ids\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUN THE PIPELINE\n",
    "# -------------------------\n",
    "\n",
    "df_cc_base, cc_seen_ids = collect_baseline_casual()\n",
    "\n",
    "df_cc_full, cc_seen_ids = keyword_expand_casual(\n",
    "    df_cc_base,\n",
    "    cc_seen_ids,\n",
    "    CASUAL_KEYWORDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge depression datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3872, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_dt</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>source</th>\n",
       "      <th>matched_keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1p9nrd6</td>\n",
       "      <td>depression</td>\n",
       "      <td>FewNewspaper5365</td>\n",
       "      <td>1.764419e+09</td>\n",
       "      <td>2025-11-29T12:27:20+00:00</td>\n",
       "      <td>I think suicide is my choice</td>\n",
       "      <td>I think it's wrong for a society to prevent pe...</td>\n",
       "      <td>I think suicide is my choice\\n\\nI think it's w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1p9nqc5</td>\n",
       "      <td>depression</td>\n",
       "      <td>pink_kitty574</td>\n",
       "      <td>1.764419e+09</td>\n",
       "      <td>2025-11-29T12:25:41+00:00</td>\n",
       "      <td>Not sure what to do</td>\n",
       "      <td>\\nI just took a lot of anxiety pills all at on...</td>\n",
       "      <td>Not sure what to do\\n\\n\\nI just took a lot of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1p9npoc</td>\n",
       "      <td>depression</td>\n",
       "      <td>NicyVicy</td>\n",
       "      <td>1.764419e+09</td>\n",
       "      <td>2025-11-29T12:24:41+00:00</td>\n",
       "      <td>Severe depression + sertralin</td>\n",
       "      <td>I don’t know what to do anymore. I have been d...</td>\n",
       "      <td>Severe depression + sertralin\\n\\nI don’t know ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1p9nf4e</td>\n",
       "      <td>depression</td>\n",
       "      <td>tommyheavenIy</td>\n",
       "      <td>1.764418e+09</td>\n",
       "      <td>2025-11-29T12:08:54+00:00</td>\n",
       "      <td>anger out of control</td>\n",
       "      <td>i'm not sure if this is the appropriate sub to...</td>\n",
       "      <td>anger out of control\\n\\ni'm not sure if this i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1p9ncyq</td>\n",
       "      <td>depression</td>\n",
       "      <td>FrostyAlarm6695</td>\n",
       "      <td>1.764418e+09</td>\n",
       "      <td>2025-11-29T12:05:29+00:00</td>\n",
       "      <td>Carelessness</td>\n",
       "      <td>Lately I’ve been very aware and pressured abou...</td>\n",
       "      <td>Carelessness\\n\\nLately I’ve been very aware an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1p9naqi</td>\n",
       "      <td>depression</td>\n",
       "      <td>Darkrose808</td>\n",
       "      <td>1.764418e+09</td>\n",
       "      <td>2025-11-29T12:01:58+00:00</td>\n",
       "      <td>These hands.</td>\n",
       "      <td>They cradle a bottle of pills. \\n\\nThe same wa...</td>\n",
       "      <td>These hands.\\n\\nThey cradle a bottle of pills....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1p9n085</td>\n",
       "      <td>depression</td>\n",
       "      <td>Due_Scene_4782</td>\n",
       "      <td>1.764417e+09</td>\n",
       "      <td>2025-11-29T11:45:09+00:00</td>\n",
       "      <td>I’m falling</td>\n",
       "      <td>For a while now I’ve been on the edge of manic...</td>\n",
       "      <td>I’m falling\\n\\nFor a while now I’ve been on th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1p9mx4c</td>\n",
       "      <td>depression</td>\n",
       "      <td>hootyowl8</td>\n",
       "      <td>1.764416e+09</td>\n",
       "      <td>2025-11-29T11:39:54+00:00</td>\n",
       "      <td>Curating a life despite anhedonia</td>\n",
       "      <td>I have been struggling with depression and anh...</td>\n",
       "      <td>Curating a life despite anhedonia\\n\\nI have be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1p9mv3r</td>\n",
       "      <td>depression</td>\n",
       "      <td>Enough-Syrup-1577</td>\n",
       "      <td>1.764416e+09</td>\n",
       "      <td>2025-11-29T11:36:27+00:00</td>\n",
       "      <td>I feel defective for having SI</td>\n",
       "      <td>I have major anxiety, depression and SI. Its t...</td>\n",
       "      <td>I feel defective for having SI\\n\\nI have major...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1p9mmr9</td>\n",
       "      <td>depression</td>\n",
       "      <td>Plenty_Answer5556</td>\n",
       "      <td>1.764415e+09</td>\n",
       "      <td>2025-11-29T11:22:35+00:00</td>\n",
       "      <td>At this point I think I just hate having friends</td>\n",
       "      <td>Always feels easier without someone else with ...</td>\n",
       "      <td>At this point I think I just hate having frien...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   subreddit             author   created_utc  \\\n",
       "0  1p9nrd6  depression   FewNewspaper5365  1.764419e+09   \n",
       "1  1p9nqc5  depression      pink_kitty574  1.764419e+09   \n",
       "2  1p9npoc  depression           NicyVicy  1.764419e+09   \n",
       "3  1p9nf4e  depression      tommyheavenIy  1.764418e+09   \n",
       "4  1p9ncyq  depression    FrostyAlarm6695  1.764418e+09   \n",
       "5  1p9naqi  depression        Darkrose808  1.764418e+09   \n",
       "6  1p9n085  depression     Due_Scene_4782  1.764417e+09   \n",
       "7  1p9mx4c  depression          hootyowl8  1.764416e+09   \n",
       "8  1p9mv3r  depression  Enough-Syrup-1577  1.764416e+09   \n",
       "9  1p9mmr9  depression  Plenty_Answer5556  1.764415e+09   \n",
       "\n",
       "                  created_dt  \\\n",
       "0  2025-11-29T12:27:20+00:00   \n",
       "1  2025-11-29T12:25:41+00:00   \n",
       "2  2025-11-29T12:24:41+00:00   \n",
       "3  2025-11-29T12:08:54+00:00   \n",
       "4  2025-11-29T12:05:29+00:00   \n",
       "5  2025-11-29T12:01:58+00:00   \n",
       "6  2025-11-29T11:45:09+00:00   \n",
       "7  2025-11-29T11:39:54+00:00   \n",
       "8  2025-11-29T11:36:27+00:00   \n",
       "9  2025-11-29T11:22:35+00:00   \n",
       "\n",
       "                                              title  \\\n",
       "0                      I think suicide is my choice   \n",
       "1                               Not sure what to do   \n",
       "2                     Severe depression + sertralin   \n",
       "3                              anger out of control   \n",
       "4                                      Carelessness   \n",
       "5                                      These hands.   \n",
       "6                                       I’m falling   \n",
       "7                 Curating a life despite anhedonia   \n",
       "8                    I feel defective for having SI   \n",
       "9  At this point I think I just hate having friends   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  I think it's wrong for a society to prevent pe...   \n",
       "1  \\nI just took a lot of anxiety pills all at on...   \n",
       "2  I don’t know what to do anymore. I have been d...   \n",
       "3  i'm not sure if this is the appropriate sub to...   \n",
       "4  Lately I’ve been very aware and pressured abou...   \n",
       "5  They cradle a bottle of pills. \\n\\nThe same wa...   \n",
       "6  For a while now I’ve been on the edge of manic...   \n",
       "7  I have been struggling with depression and anh...   \n",
       "8  I have major anxiety, depression and SI. Its t...   \n",
       "9  Always feels easier without someone else with ...   \n",
       "\n",
       "                                           full_text  score  num_comments  \\\n",
       "0  I think suicide is my choice\\n\\nI think it's w...      1             0   \n",
       "1  Not sure what to do\\n\\n\\nI just took a lot of ...      1             0   \n",
       "2  Severe depression + sertralin\\n\\nI don’t know ...      1             0   \n",
       "3  anger out of control\\n\\ni'm not sure if this i...      1             0   \n",
       "4  Carelessness\\n\\nLately I’ve been very aware an...      1             0   \n",
       "5  These hands.\\n\\nThey cradle a bottle of pills....      1             0   \n",
       "6  I’m falling\\n\\nFor a while now I’ve been on th...      1             1   \n",
       "7  Curating a life despite anhedonia\\n\\nI have be...      1             0   \n",
       "8  I feel defective for having SI\\n\\nI have major...      1             0   \n",
       "9  At this point I think I just hate having frien...      1             0   \n",
       "\n",
       "                                                 url link_flair_text  \\\n",
       "0  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "1  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "2  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "3  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "4  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "5  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "6  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "7  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "8  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "9  https://www.reddit.com/r/depression/comments/1...             NaN   \n",
       "\n",
       "     source matched_keyword  \n",
       "0  baseline             NaN  \n",
       "1  baseline             NaN  \n",
       "2  baseline             NaN  \n",
       "3  baseline             NaN  \n",
       "4  baseline             NaN  \n",
       "5  baseline             NaN  \n",
       "6  baseline             NaN  \n",
       "7  baseline             NaN  \n",
       "8  baseline             NaN  \n",
       "9  baseline             NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the data sets together\n",
    "df_dep_base = pd.read_csv(\"depression_baseline_1000.csv\")\n",
    "df_dep_kw   = pd.read_csv(\"depression_expanded_keywords.csv\")\n",
    "\n",
    "# merge \n",
    "df_dep = pd.concat([df_dep_base, df_dep_kw], ignore_index=True)\n",
    "\n",
    "# drop duplicates by Reddit post ID\n",
    "df_dep = df_dep.drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "\n",
    "print(df_dep.shape)\n",
    "\n",
    "df_dep.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depression dataset: (3872, 15)\n",
      "CasualConversation dataset: (4051, 15)\n",
      "Original rows: 7923\n",
      "Cleaned rows: 7831\n",
      "Duplicates removed: 12\n",
      "Saved final cleaned file: clean_reddit.csv\n"
     ]
    }
   ],
   "source": [
    "### ------------------------------\n",
    "### 1. Load & merge depression\n",
    "### ------------------------------\n",
    "\n",
    "df_dep_base = pd.read_csv(\"depression_baseline_1000.csv\")\n",
    "df_dep_kw   = pd.read_csv(\"depression_expanded_keywords.csv\")\n",
    "\n",
    "# Merge + remove ID duplicates\n",
    "df_dep = pd.concat([df_dep_base, df_dep_kw], ignore_index=True)\n",
    "df_dep = df_dep.drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "\n",
    "# Label depression\n",
    "df_dep[\"label\"] = 1\n",
    "\n",
    "print(\"Depression dataset:\", df_dep.shape)\n",
    "\n",
    "\n",
    "### ------------------------------\n",
    "### 2. Load & merge CasualConversation\n",
    "### ------------------------------\n",
    "\n",
    "df_cc_base = pd.read_csv(\"casualconversation_baseline_1000.csv\")\n",
    "df_cc_kw   = pd.read_csv(\"casualconversation_expanded_keywords.csv\")\n",
    "\n",
    "df_cc = pd.concat([df_cc_base, df_cc_kw], ignore_index=True)\n",
    "df_cc = df_cc.drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "\n",
    "# Label casual\n",
    "df_cc[\"label\"] = 0\n",
    "\n",
    "print(\"CasualConversation dataset:\", df_cc.shape)\n",
    "\n",
    "\n",
    "### ------------------------------\n",
    "### 3. Combine, dedupe, final clean\n",
    "### ------------------------------\n",
    "\n",
    "df_final = pd.concat([df_dep, df_cc], ignore_index=True)\n",
    "df_final = df_final.drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "\n",
    "### Remove moderator/bot posts\n",
    "bot_authors = [\"AutoModerator\", \"[deleted]\", \"moderator\", \"ModTeam\"]\n",
    "df_final = df_final[~df_final[\"author\"].str.lower().isin([a.lower() for a in bot_authors])]\n",
    "\n",
    "### Optional: remove removal messages\n",
    "removal_patterns = [\"removed\", \"deleted\", \"sorry, this post was removed\"]\n",
    "df_final = df_final[~df_final[\"full_text\"].str.lower().str.contains(\"|\".join(removal_patterns))]\n",
    "\n",
    "### 4. Remove textual duplicates\n",
    "df_final_clean = df_final.drop_duplicates(subset=\"full_text\").reset_index(drop=True)\n",
    "\n",
    "df_final_clean.to_csv(\"clean_reddit.csv\", index=False)\n",
    "\n",
    "\n",
    "### ------------------------------\n",
    "### 4. Remove textual duplicates\n",
    "### ------------------------------\n",
    "\n",
    "print(\"Original rows:\", len(df))\n",
    "\n",
    "# Drop duplicates by text\n",
    "df_final_clean = df_final.drop_duplicates(subset=\"full_text\").reset_index(drop=True)\n",
    "\n",
    "print(\"Cleaned rows:\", len(df_final_clean))\n",
    "print(\"Duplicates removed:\", len(df_final) - len(df_final_clean))\n",
    "\n",
    "# Save final deduped dataset\n",
    "df_final_clean.to_csv(\"clean_reddit.csv\", index=False)\n",
    "\n",
    "print(\"Saved final cleaned file: clean_reddit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
